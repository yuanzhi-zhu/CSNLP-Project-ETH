{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"JfgX6lC-CeEN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655732778812,"user_tz":-120,"elapsed":13799,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"}},"outputId":"0722b476-8bb9-481f-ae20-e512623ce423"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers==4.19.2 in /usr/local/lib/python3.7/dist-packages (4.19.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.2) (21.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.2) (4.64.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.2) (1.21.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.2) (3.7.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.2) (4.11.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.2) (2.23.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.2) (0.12.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.2) (6.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.2) (0.7.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.2) (2022.6.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.19.2) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.19.2) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.19.2) (3.8.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.2) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.2) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.2) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.2) (2022.6.15)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: rouge_metric in /usr/local/lib/python3.7/dist-packages (1.0.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk==3.6.5 in /usr/local/lib/python3.7/dist-packages (3.6.5)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.5) (1.1.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.5) (2022.6.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.5) (4.64.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.5) (7.1.2)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n","Drive already mounted at ./mydata; to attempt to forcibly remount, call drive.mount(\"./mydata\", force_remount=True).\n"]}],"source":["!pip install transformers==4.19.2\n","!pip install rouge_metric\n","!pip install nltk==3.6.5\n","!pip install sentencepiece\n","from google.colab import drive\n","drive.mount('./mydata')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"wyzGkzsuHNAA","executionInfo":{"status":"ok","timestamp":1655732778813,"user_tz":-120,"elapsed":7,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"}}},"outputs":[],"source":["import sys\n","sys.path.append('./mydata/MyDrive/CSNLP_Project/Bert_model_COQA')\n","sys.path.append('./mydata/MyDrive/CSNLP_Project/T5_model_COQAR')\n","sys.path.append('./mydata/MyDrive/CSNLP_Project/T5_model_COQAR/rewriting')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"UZAJcRiiEZqp","executionInfo":{"status":"ok","timestamp":1655732783959,"user_tz":-120,"elapsed":5152,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"}}},"outputs":[],"source":["import collections\n","import glob\n","import os\n","import torch\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm, trange\n","from transformers import (AdamW, AutoConfig, AutoTokenizer, get_linear_schedule_with_warmup, BertTokenizer, BertModel, BertConfig)\n","from data.processors.coqa import Extract_Features, Processor, Result\n","from data.processors.evaluate import CoQAEvaluator, parse_args\n","from data.processors.Bert_model import BertBaseUncasedModel, load_dataset, Write_predictions\n","\n","#from transformers import BertModel, BertPreTrainedModel\n","\n","import csv\n","import numpy as np\n","\n","import evaluation\n","import argparse\n","import qrdatasets\n","import models\n","from utils import *\n","import random\n","import t5small\n","import t5base\n","import nltk\n","\n","import config\n","import json\n","\n","# CoQA dataset file\n","train_file=\"coqa-train-v1.0.json\"\n","predict_file=\"coqa-dev-v1.0.json\"\n","cur_path = os.getcwd()\n","output_directory = cur_path + \"/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/Bert_models\"\n","input_dir = cur_path + \"/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data\"\n","# can use either BERT base or BERT large\n","pretrained_model=\"bert-base-uncased\"\n","# pretrained_model=\"bert-large-uncased\"\n","# it's better to fine-tune Bert-base for 4 epoches than only one\n","epochs = 4\n","evaluation_batch_size=1\n","train_batch_size=2"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ZyM0goV04hKB","executionInfo":{"status":"ok","timestamp":1655732797295,"user_tz":-120,"elapsed":13339,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"}}},"outputs":[],"source":["### Load two fine-tuned models, but with diffetent load methods\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# load bert\n","# bert_path = './mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/Bert_models/Bert_from_original_Surya_epoch4'\n","# bert_model = BertBaseUncasedModel.from_pretrained(bert_path) \n","# bert_tokenizer = BertTokenizer.from_pretrained(bert_path, do_lower_case=True)\n","# bert_model.to(device)\n","\n","# load t5\n","# t5_path = './mydata/MyDrive/CSNLP_Project/T5_model_COQAR/trained_models/t5_small_with_story_batch16_hist_3_mixed/epoch10'\n","t5_path = './mydata/MyDrive/CSNLP_Project/T5_model_COQAR/trained_models/t5_base_with_story_batch4_hist_20/epoch6.zip'\n","\n","t5_model = torch.load(t5_path)\n","# t5_input_tokenizer = t5small.get_input_tokenizer()\n","# t5_output_tokenizer = t5small.get_output_tokenizer()\n","t5_input_tokenizer = t5base.get_input_tokenizer()\n","t5_output_tokenizer = t5base.get_output_tokenizer()\n","t5_model.to(device)\n","\n","pass"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1655732052689,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"EVlsaahMN_EZ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a2d1b314-386b-40bd-89b4-42b956d28f44"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}],"source":["# default parameters for T5\n","hparams = {\n","    'epochs' : 3,\n","    'learning_rate' : 0.00005,\n","    'batch_size' : 16,\n","    'weight_decay' : 0.0,\n","    'history_size' : 3,\n","    'dropout_rate' : 0.1,\n","    'include_story' : True,\n","    'model_size' : 'small'\n","}\n","nltk.download('wordnet')"]},{"cell_type":"code","source":["# get rewritten answers from T5\n","\n","def generate_new_dataset(input_dir,file_type='train',t5_type='base',t5_model=t5_model,method='append'):\n","### open the COQA dataset\n","    if file_type == 'train':\n","        # training dataset\n","        with open(os.path.join(input_dir, train_file), \"r\", encoding=\"utf-8\") as reader:\n","            input_data = json.load(reader)\n","    elif file_type == 'dev':\n","        # dev dataset\n","        with open(os.path.join(input_dir, predict_file), \"r\", encoding=\"utf-8\") as reader:\n","            input_data = json.load(reader)\n","    else:\n","        raise Exception(\"must specify a file type: train/dev\")\n","        pass\n","\n","    # create empty dictionary\n","    t5_data = {'input':[], 'references':[], 'context':[]}\n","\n","    # create dataset for T5\n","    for input_d in input_data[\"data\"]:\n","        questions = []\n","        assert len(input_d['questions']) == len(input_d['answers'])\n","        for id_q in range(len(input_d['questions'])):\n","            if id_q == 0:\n","                questions.append(input_d['questions'][id_q]['input_text'])\n","            else:\n","                questions.append(input_d['answers'][id_q-1]['input_text'])\n","                questions.append(input_d['questions'][id_q]['input_text'])\n","            t5_data['input'].append(questions.copy())\n","            t5_data['references'].append([''])\n","            t5_data['context'].append(input_d['story'])\n","\n","    # T5 make dataset\n","    if t5_type == 'base':\n","        dataset = t5base.make_dataset(t5_data, hparams, cuda = True)\n","    elif t5_type == 'small':\n","        dataset = t5base.make_dataset(t5_data, hparams, cuda = True)\n","    else:\n","        raise Exception(\"must specify a t5 model type: base/small\")\n","        pass\n","\n","    # generate rewritten questions\n","    loader = DataLoader(dataset=dataset, batch_size=hparams['batch_size'])\n","    t5_model.cuda()\n","    t5_model.train(False)\n","    rewritten_qs = []\n","    for dic in loader:\n","        output = t5_model.generate(input_ids = dic['input_ids'], attention_mask = dic['attention_mask'])\n","        pred = t5_output_tokenizer.batch_decode(output, skip_special_tokens = True)\n","        rewritten_qs += pred\n","\n","    ### Create new training/dev dataset json file for Bert\n","    # append or replace with the generated question \n","    count = 0\n","    for story_id in range(len(input_data[\"data\"])):\n","        for question_id in range(len(input_data[\"data\"][story_id]['questions'])):\n","            if method == 'append':\n","                # append the rewritten question\n","                input_data[\"data\"][story_id]['questions'][question_id]['input_text'] += (' '+rewritten_qs[count])\n","            elif method == 'replace':\n","                # replace with the rewritten question\n","                input_data[\"data\"][story_id]['questions'][question_id]['input_text'] = rewritten_qs[count]\n","            else:\n","                raise Exception(\"must specify a method: append/replace\")\n","                pass\n","            count += 1\n","\n","    # save as new dataset json file\n","    file_name = 'coqa-{}-v1.0-{}_with_T5.json'.format(file_type,method)\n","    # with open(os.path.join( input_dir, 'coqa-dev-v1.0-with_T5.json'), 'w', encoding=\"utf-8\") as outfile:\n","    with open(os.path.join(input_dir, file_name), 'w', encoding=\"utf-8\") as outfile:\n","        json.dump(input_data, outfile)\n","\n","\n","generate_new_dataset(input_dir,file_type='dev',t5_type='base',t5_model=t5_model,method='replace')"],"metadata":{"id":"fzxnuGYN1cJl","executionInfo":{"status":"ok","timestamp":1655732054932,"user_tz":-120,"elapsed":32,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XDyYsqyW9JEY"},"source":["## Prediction\n","\n","predict on dev dataset"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f-lZDiEb4OcD","executionInfo":{"status":"ok","timestamp":1655732797296,"user_tz":-120,"elapsed":11,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"}},"outputId":"6bcd36c6-814b-4844-beb9-4067572405b9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/Bert_models/Bert_from_original_Surya_epoch4',\n"," '/content/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/Bert_models/Bert_with_T5_rewritten_epoch4',\n"," '/content/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/Bert_models/Bert_with_T5_rewritten_epoch4_replace']"]},"metadata":{},"execution_count":5}],"source":["### Load two fine-tuned models, but with diffetent load methods\n","\n","model_parameter_directory = [ f.path for f in os.scandir(output_directory) if f.is_dir() ]\n","\n","method = '_replace'\n","\n","# use catch file name\n","cache_file_name = 'bert-base-uncased_dev_with_T5{}'.format(method)\n","# use the predict file name\n","predict_file_name = 'coqa-dev-v1.0-{}_with_T5.json'.format(method[1:])\n","# # reset the catch file name\n","# cache_file_name = None\n","# # reset the predict file name\n","# predict_file_name = None\n","\n","model_parameter_directory"]},{"cell_type":"code","source":["# for m in model_parameter_directory:\n","m = model_parameter_directory[2]\n","variant_name = m.split('/')[-1]\n","print(variant_name)\n","# m = m + '/pytorch_model_2.bin'\n","model = BertBaseUncasedModel.from_pretrained(m) \n","tokenizer = BertTokenizer.from_pretrained(m, do_lower_case=True)\n","model.to(device)\n","Write_predictions(model, tokenizer, device, variant_name, input_dir=input_dir, output_directory=output_directory, cache_file_name=cache_file_name, predict_file_name=predict_file_name, method=method)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"quccxYdDyZNp","executionInfo":{"status":"ok","timestamp":1655733179945,"user_tz":-120,"elapsed":382655,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"}},"outputId":"44c3e7c9-5590-4067-d38c-c94cb7e10ffe"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Bert_with_T5_rewritten_epoch4_replace\n","Loading cache /content/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/bert-base-uncased_dev_with_T5_replace\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 8636/8636 [05:45<00:00, 24.97it/s]\n","Writing preditions: 100%|██████████| 7983/7983 [00:28<00:00, 284.16it/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"L_AFYP9k0Yu1"},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4VY9u1rP0cSu"},"outputs":[],"source":["evaluator = CoQAEvaluator(input_dir+'/'+predict_file_name)\n","\n","variant_name = 'Bert_with_T5_rewritten_epoch4_replace'\n","\n","pre_file_bert = output_directory+'/'+variant_name+'/'+'predictions{}.json'.format(method)\n","\n","# evaluate\n","with open(pre_file_bert) as f:\n","    pred_data = CoQAEvaluator.preds_to_dict(pre_file_bert)\n","\n","# write evaluate results\n","with open(output_directory+'/'+variant_name+'/'+'evaluation{}.json'.format(method), 'w') as f:\n","    json.dump(evaluator.model_performance(pred_data), f, indent=2)\n","\n","# show\n","print(json.dumps(evaluator.model_performance(pred_data), indent=2))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"BertWithT5.ipynb","provenance":[]},"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"},"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}