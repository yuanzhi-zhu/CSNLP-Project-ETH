{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18690,"status":"ok","timestamp":1656632376489,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"JfgX6lC-CeEN","outputId":"b6e4639e-a7e4-4144-b351-ea53171868f8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers==4.19.2 in /usr/local/lib/python3.7/dist-packages (4.19.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.2) (4.11.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.2) (2022.6.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.2) (0.8.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.2) (3.7.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.2) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.2) (2.23.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.2) (0.12.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.2) (4.64.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.2) (6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.2) (1.21.6)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.19.2) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.19.2) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.19.2) (3.8.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.2) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.2) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.2) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.2) (2.10)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: rouge_metric in /usr/local/lib/python3.7/dist-packages (1.0.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk==3.6.5 in /usr/local/lib/python3.7/dist-packages (3.6.5)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.5) (7.1.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.5) (4.64.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.5) (1.1.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.5) (2022.6.2)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n","Drive already mounted at ./mydata; to attempt to forcibly remount, call drive.mount(\"./mydata\", force_remount=True).\n"]}],"source":["!pip install transformers==4.19.2\n","!pip install rouge_metric\n","!pip install nltk==3.6.5\n","!pip install sentencepiece\n","from google.colab import drive\n","drive.mount('./mydata')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1656632376491,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"wyzGkzsuHNAA"},"outputs":[],"source":["import sys\n","sys.path.append('./mydata/MyDrive/CSNLP_Project/Bert_model_COQA')\n","sys.path.append('./mydata/MyDrive/CSNLP_Project/T5_model_COQAR')\n","sys.path.append('./mydata/MyDrive/CSNLP_Project/T5_model_COQAR/rewriting')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":11392,"status":"ok","timestamp":1656632387868,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"UZAJcRiiEZqp"},"outputs":[],"source":["import collections\n","import glob\n","import os\n","import torch\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from tqdm import tqdm, trange\n","from transformers import (AdamW, AutoConfig, AutoTokenizer, get_linear_schedule_with_warmup, BertTokenizer, BertModel, BertConfig)\n","from processors.coqa import Extract_Features, Processor, Result\n","from processors.evaluate import CoQAEvaluator, parse_args\n","from processors.Bert_model import BertBaseUncasedModel, BertBaseUncasedModel_with_T5, load_dataset, Write_predictions\n","\n","#from transformers import BertModel, BertPreTrainedModel\n","\n","import csv\n","import numpy as np\n","\n","import evaluation\n","import argparse\n","import qrdatasets\n","import models\n","from utils import *\n","import random\n","import t5small\n","import t5base\n","import nltk\n","\n","import config\n","import json\n","\n","# CoQA dataset file\n","train_file=\"coqa-train-v1.0.json\"\n","predict_file=\"coqa-dev-v1.0.json\"\n","cur_path = os.getcwd()\n","output_directory = cur_path + \"/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/Bert_models\"\n","input_dir = cur_path + \"/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data\"\n","# can use either BERT base or BERT large\n","pretrained_model=\"bert-base-uncased\"\n","# pretrained_model=\"bert-large-uncased\"\n","# it's better to fine-tune Bert-base for 4 epoches than only one\n","epochs = 4\n","evaluation_batch_size=1\n","train_batch_size=2"]},{"cell_type":"markdown","metadata":{"id":"jvCeeKjL1PBp"},"source":["### create now dataset (json file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZyM0goV04hKB"},"outputs":[],"source":["### Load two fine-tuned models, but with diffetent load methods\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# load bert\n","# bert_path = './mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/Bert_models/Bert_from_original_Surya_epoch4'\n","# bert_model = BertBaseUncasedModel.from_pretrained(bert_path) \n","# bert_tokenizer = BertTokenizer.from_pretrained(bert_path, do_lower_case=True)\n","# bert_model.to(device)\n","\n","# load t5\n","# t5_path = './mydata/MyDrive/CSNLP_Project/T5_model_COQAR/trained_models/t5_small_with_story_batch16_hist_3_mixed/epoch10'\n","t5_path = './mydata/MyDrive/CSNLP_Project/T5_model_COQAR/trained_models/t5_base_with_story_batch4_hist_20/epoch6.zip'\n","\n","t5_model = torch.load(t5_path)\n","# t5_input_tokenizer = t5small.get_input_tokenizer()\n","# t5_output_tokenizer = t5small.get_output_tokenizer()\n","t5_input_tokenizer = t5base.get_input_tokenizer()\n","t5_output_tokenizer = t5base.get_output_tokenizer()\n","t5_model.to(device)\n","\n","pass"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1656632387869,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"EVlsaahMN_EZ","outputId":"d6320004-a84c-4fd0-c8b6-e3d1e9605226"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# default parameters for T5\n","hparams = {\n","    'epochs' : 3,\n","    'learning_rate' : 0.00005,\n","    'batch_size' : 16,\n","    'weight_decay' : 0.0,\n","    'history_size' : 3,\n","    'dropout_rate' : 0.1,\n","    'include_story' : True,\n","    'model_size' : 'small'\n","}\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1656632388217,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"cSwK3c3Rp3uo"},"outputs":[],"source":["def make_t5_dataset(input_dir,file_type='train',t5_type='base',train_file=train_file):\n","    ### open the COQA dataset\n","    if file_type == 'train':\n","        # training dataset\n","        with open(os.path.join(input_dir, train_file), \"r\", encoding=\"utf-8\") as reader:\n","            input_data = json.load(reader)\n","    elif file_type == 'dev':\n","        # dev dataset\n","        with open(os.path.join(input_dir, predict_file), \"r\", encoding=\"utf-8\") as reader:\n","            input_data = json.load(reader)\n","    else:\n","        raise Exception(\"must specify a file type: train/dev\")\n","        pass\n","    \n","    # create empty dictionary\n","    t5_data = {'input':[], 'references':[], 'context':[]}\n","    # create dataset for T5\n","    for input_d in input_data[\"data\"]:\n","        questions = []\n","        assert len(input_d['questions']) == len(input_d['answers'])\n","        for id_q in range(len(input_d['questions'])):\n","            curr_question = input_d['questions'][id_q]['input_text']\n","            if '?' not in curr_question:\n","                if len(curr_question) == 0:\n","                    pass\n","                # some question in statement form end with '.'\n","                elif curr_question[-1] in ['.']:\n","                    if curr_question[-2:] == '/.':\n","                        curr_question = curr_question[:-2] + '?'\n","                    elif ('True or False' in curr_question) or ('Name' in curr_question):\n","                        pass\n","                    else:\n","                        # not worth to deal with such small portion of data\n","                        pass\n","                elif curr_question[-1] in ['/']:\n","                    curr_question = curr_question[:-1] + '?'\n","                else:\n","                    curr_question += '?'\n","            if id_q == 0:\n","                questions.append(curr_question)\n","            else:\n","                questions.append(input_d['answers'][id_q-1]['input_text'])\n","                questions.append(curr_question)\n","            t5_data['input'].append(questions.copy())\n","            t5_data['references'].append([''])\n","            t5_data['context'].append(input_d['story'])\n","\n","    # T5 make dataset\n","    if t5_type == 'base':\n","        dataset = t5base.make_dataset(t5_data, hparams, cuda = True)\n","    elif t5_type == 'small':\n","        dataset = t5base.make_dataset(t5_data, hparams, cuda = True)\n","    else:\n","        raise Exception(\"must specify a t5 model type: base/small\")\n","    \n","    return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1o7NxaOY6VcT"},"outputs":[],"source":["# get rewritten answers from T5\n","\n","def generate_new_dataset(input_dir,file_type='train',t5_type='base',t5_model=t5_model,method='append'):\n","    ### open the COQA dataset\n","    if file_type == 'train':\n","        # training dataset\n","        with open(os.path.join(input_dir, train_file), \"r\", encoding=\"utf-8\") as reader:\n","            input_data = json.load(reader)\n","    elif file_type == 'dev':\n","        # dev dataset\n","        with open(os.path.join(input_dir, predict_file), \"r\", encoding=\"utf-8\") as reader:\n","            input_data = json.load(reader)\n","    else:\n","        raise Exception(\"must specify a file type: train/dev\")\n","        pass\n","        \n","    if 'repeat' not in method:\n","        # get dataset\n","        dataset = make_t5_dataset(input_dir,file_type=file_type,t5_type=t5_type,train_file=train_file)\n","\n","        ### generate rewritten questions\n","        loader = DataLoader(dataset=dataset, batch_size=hparams['batch_size'])\n","        t5_model.cuda()\n","        t5_model.train(False)\n","        rewritten_qs = []\n","        for dic in tqdm(loader, desc=\"Generating rewritten questions\"):\n","            output = t5_model.generate(input_ids = dic['input_ids'], attention_mask = dic['attention_mask'])\n","            pred = t5_output_tokenizer.batch_decode(output, skip_special_tokens = True)\n","            rewritten_qs += pred\n","\n","    ### Create new training/dev dataset json file for Bert\n","    # append or replace with the generated question \n","    count = 0\n","    for story_id in range(len(input_data[\"data\"])):\n","        for question_id in range(len(input_data[\"data\"][story_id]['questions'])):\n","            # if the rewritten question from T5 is failed, we use the original question.\n","\n","            curr_question = input_data[\"data\"][story_id]['questions'][question_id]['input_text']\n","            if '?' not in curr_question:\n","                if len(curr_question) == 0:\n","                    pass\n","                # some question in statement form end with '.'\n","                elif curr_question[-1] in ['.']:\n","                    if curr_question[-2:] == '/.':\n","                        curr_question = curr_question[:-2] + '?'\n","                    elif ('True or False' in curr_question) or ('Name' in curr_question):\n","                        pass\n","                    else:\n","                        # not worth to deal with such small portion of data\n","                        pass\n","                elif curr_question[-1] in ['/']:\n","                    curr_question = curr_question[:-1] + '?'\n","                else:\n","                    curr_question += '?'\n","\n","            if 'repeat' not in method:\n","                if len(rewritten_qs[count]) < 2:\n","                    pass\n","                # for failed rewritten questions(those not even questions)\n","                elif rewritten_qs[count][-1] != '?':\n","                    rewritten_qs[count] = curr_question\n","            if 'append' in method:\n","                # append the rewritten question\n","                input_data[\"data\"][story_id]['questions'][question_id]['input_text'] = curr_question + (' '+rewritten_qs[count])\n","            elif 'replace' in method:\n","                # replace with the rewritten question\n","                input_data[\"data\"][story_id]['questions'][question_id]['input_text'] = rewritten_qs[count]\n","            elif 'repeat' in method:\n","                # repeat the curent question twice for comparison\n","                input_data[\"data\"][story_id]['questions'][question_id]['input_text'] = curr_question + (' '+curr_question)\n","            else:\n","                raise Exception(\"must specify a method: append/replace\")\n","                pass\n","            count += 1\n","\n","    # save as new dataset json file\n","    file_name = 'coqa-{}-v1.0-{}_with_T5.json'.format(file_type,method)\n","    # with open(os.path.join( input_dir, 'coqa-dev-v1.0-with_T5.json'), 'w', encoding=\"utf-8\") as outfile:\n","    with open(os.path.join(input_dir, file_name), 'w', encoding=\"utf-8\") as outfile:\n","        json.dump(input_data, outfile)\n","\n","generate_new_dataset(input_dir,file_type='dev',t5_type='base',t5_model=t5_model,method='append_v3')"]},{"cell_type":"markdown","metadata":{"id":"fFtnkFKgvWXh"},"source":["### train bert with t5 embedding"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1656632388218,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"lymNPhfWl7-E"},"outputs":[],"source":["def train_bert_with_t5(bert_model, t5_model, bert_tokenizer, bert_dataset, t5_dataset, device, batch_size=4):\n","\n","    bert_train_sampler = SequentialSampler(bert_dataset) \n","    bert_dataloader = DataLoader(bert_dataset, sampler=bert_train_sampler, batch_size=batch_size)\n","    t5_dataloader = DataLoader(dataset=t5_dataset, batch_size=batch_size)\n","    t_total = len(bert_dataloader) // 1 * epochs\n","\n","    # Preparing optimizer and scheduler\n","    optimizer_parameters = [{\"params\": [p for n, p in bert_model.named_parameters() if not any(nd in n for nd in [\"bias\", \"LayerNorm.weight\"])],\"weight_decay\": 0.01,},{\"params\": [p for n, p in bert_model.named_parameters() if any(nd in n for nd in [\"bias\", \"LayerNorm.weight\"])], \"weight_decay\": 0.0}]\n","    optimizer = AdamW(optimizer_parameters,lr=1e-5, eps=1e-8)\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=2000, num_training_steps=t_total)\n","\n","    # Check if saved optimizer or scheduler states exist\n","    if os.path.isfile(os.path.join(pretrained_model, \"optimizer.pt\")) and os.path.isfile(os.path.join(pretrained_model, \"scheduler.pt\")):\n","        optimizer.load_state_dict(torch.load(\n","            os.path.join(pretrained_model, \"optimizer.pt\")))\n","        scheduler.load_state_dict(torch.load(\n","            os.path.join(pretrained_model, \"scheduler.pt\")))\n","\n","    counter = 1\n","    epochs_trained = 0\n","    train_loss, loss = 0.0, 0.0\n","    bert_model.zero_grad()\n","    iterator = trange(epochs_trained, int(epochs), desc=\"Epoch\", disable=False)\n","    for _ in iterator:\n","        epoch_iterator = tqdm(zip(bert_dataloader,t5_dataloader), desc=\"Iteration\", disable=True)\n","        for i,batch in enumerate(epoch_iterator):\n","            batch_bert, batch_t5 = batch\n","            # model.encoder(input_ids=s, attention_mask=attn, return_dict=True)\n","            # pooled_sentence = output.last_hidden_state # shape is [batch_size, seq_len, hidden_size]\n","            # # pooled_sentence will represent the embeddings for each word in the sentence\n","            # # you need to sum/average the pooled_sentence\n","            # pooled_sentence = torch.mean(pooled_sentence, dim=1)\n","            t5_embdeding = t5_model.encoder(input_ids = batch_t5['input_ids'], attention_mask = batch_t5['attention_mask'], return_dict=False)\n","            bert_model.train()\n","            batch_bert = tuple(t.to(device) for t in batch_bert)\n","            # inputs = { \"input_ids\": batch_bert[0],\"token_type_ids\": batch_bert[1], \"attention_mask\": batch_bert[2],\"start_positions\": batch_bert[3],\"end_positions\": batch_bert[4],\"rational_mask\": batch_bert[5],\"cls_idx\": batch_bert[6]}\n","            # loss = bert_model(**inputs)\n","            loss = bert_model(batch_bert[0],t5_embdeding,batch_t5,token_type_ids=batch_bert[1],attention_mask=batch_bert[2],start_positions=batch_bert[3],end_positions=batch_bert[4],rational_mask=batch_bert[5],cls_idx=batch_bert[6],head_mask=None)\n","            loss.backward()\n","            train_loss += loss.item()\n","\n","            #   optimizing training parameters\n","            if (i + 1) % 1 == 0:\n","                optimizer.step()\n","                scheduler.step()  \n","                bert_model.zero_grad()\n","                counter += 1\n","                #   Saving model weights every 1000 iterations\n","                if counter % 1000 == 0:\n","                    output_dir = os.path.join(output_directory, \"model_weights\"+str(epochs_trained))\n","                    if not os.path.exists(output_dir):\n","                        os.makedirs(output_dir)\n","                    model_to_save = bert_model.module if hasattr(bert_model, \"module\") else bert_model\n","                    model_to_save.save_pretrained(output_dir)\n","                    tokenizer.save_pretrained(output_dir)\n","                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n","                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n","            if (i+1) % 1000 == 0:\n","                print('iter: {}, loss: {}'.format(i,train_loss/counter))\n","    return train_loss/counter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y8kuWh347W1-"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","method = 'append'\n","epochs = 4\n","\n","## load raw model\n","# t5 \n","t5_path = './mydata/MyDrive/CSNLP_Project/T5_model_COQAR/trained_models/t5_base_with_story_batch4_hist_20/epoch6.zip'\n","t5_model = torch.load(t5_path)\n","t5_input_tokenizer = t5base.get_input_tokenizer()   # or t5small \n","t5_output_tokenizer = t5base.get_output_tokenizer()\n","t5_model.to(device)\n","t5_model.train(False)\n","# bert\n","config = BertConfig.from_pretrained(pretrained_model, return_dict=False)\n","bert_tokenizer = BertTokenizer.from_pretrained(pretrained_model)\n","bert_model = BertBaseUncasedModel_with_T5.from_pretrained(pretrained_model, from_tf=bool(\".ckpt\" in pretrained_model), config=config,cache_dir=None,)\n","bert_model.to(device)\n","\n","## get datasets\n","t5_dataset = make_t5_dataset(input_dir,file_type='train',t5_type='base',train_file=train_file)\n","cache_file_name = 'bert-base-uncased_train_with_T5_{}_v3.2'.format(method)\n","train_file_name = 'coqa-train-v1.0-{}_v3_with_T5.json'.format(method)\n","bert_dataset = load_dataset(bert_tokenizer, input_dir=input_dir, evaluate=False, cache_file_name=cache_file_name, train_file_name=train_file_name, append_method='append')\n","\n","# train\n","train_bert_with_t5(bert_model, t5_model, bert_tokenizer, bert_dataset, t5_dataset, device, batch_size=4)"]},{"cell_type":"markdown","metadata":{"id":"XDyYsqyW9JEY"},"source":["## Prediction\n","\n","predict on dev dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1656583621214,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"f-lZDiEb4OcD","outputId":"73c12b71-2833-4f84-815b-4278bb2c768f"},"outputs":[{"data":{"text/plain":["['/content/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/Bert_models/Bert_from_original_Surya_epoch4',\n"," '/content/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/Bert_models/Bert_with_T5_rewritten_epoch4_append_v2',\n"," '/content/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/Bert_models/Bert_with_T5_rewritten_epoch4_replace',\n"," '/content/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/Bert_models/Bert_with_T5_rewritten_epoch4_append_v3']"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["### Load two fine-tuned models, but with diffetent load methods\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model_parameter_directory = [ f.path for f in os.scandir(output_directory) if f.is_dir() ]\n","\n","# method = '_replace_v2'\n","method = '_append_v3'\n","\n","# use catch file name\n","cache_file_name = 'bert-base-uncased_dev_with_T5{}'.format(method)\n","# use the predict file name\n","predict_file_name = 'coqa-dev-v1.0-{}_with_T5.json'.format(method[1:])\n","# # reset the catch file name\n","# cache_file_name = None\n","# # reset the predict file name\n","# predict_file_name = None\n","\n","model_parameter_directory"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"quccxYdDyZNp"},"outputs":[],"source":["# for m in model_parameter_directory:\n","m = model_parameter_directory[3]\n","variant_name = m.split('/')[-1]\n","print(variant_name)\n","# m = m + '/pytorch_model_2.bin'\n","model = BertBaseUncasedModel.from_pretrained(m) \n","tokenizer = BertTokenizer.from_pretrained(m, do_lower_case=True)\n","model.to(device)\n","Write_predictions(model, tokenizer, device, variant_name, input_dir=input_dir, output_directory=output_directory, cache_file_name=cache_file_name, predict_file_name=predict_file_name, method=method, append_method='append')"]},{"cell_type":"markdown","metadata":{"id":"L_AFYP9k0Yu1"},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4VY9u1rP0cSu"},"outputs":[],"source":["evaluator = CoQAEvaluator(input_dir+'/'+predict_file_name)\n","\n","# variant_name = 'Bert_with_T5_rewritten_epoch4_append_v2'\n","\n","# variant_name = 'Bert_from_original_Surya_epoch4'\n","\n","variant_name = 'Bert_with_T5_rewritten_epoch4_replace'\n","\n","\n","pre_file_bert = output_directory+'/'+variant_name+'/'+'predictions{}.json'.format(method)\n","\n","# evaluate\n","with open(pre_file_bert) as f:\n","    pred_data = CoQAEvaluator.preds_to_dict(pre_file_bert)\n","\n","# write evaluate results\n","with open(output_directory+'/'+variant_name+'/'+'evaluation{}.json'.format(method), 'w') as f:\n","    json.dump(evaluator.model_performance(pred_data), f, indent=2)\n","\n","# show\n","print(json.dumps(evaluator.model_performance(pred_data), indent=2))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"BertWithT5.ipynb","provenance":[]},"gpuClass":"standard","interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"},"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}
