{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"JfgX6lC-CeEN"},"outputs":[],"source":["!pip install transformers==4.19.2\n","!pip install rouge_metric\n","!pip install nltk==3.6.5\n","!pip install sentencepiece\n","from google.colab import drive\n","drive.mount('./mydata')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wyzGkzsuHNAA"},"outputs":[],"source":["import sys\n","sys.path.append('./mydata/MyDrive/CSNLP_Project/Bert_model_COQA')\n","sys.path.append('./mydata/MyDrive/CSNLP_Project/T5_model_COQAR')\n","sys.path.append('./mydata/MyDrive/CSNLP_Project/T5_model_COQAR/rewriting')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UZAJcRiiEZqp"},"outputs":[],"source":["import collections\n","import glob\n","import os\n","import torch\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from tqdm import tqdm, trange\n","from transformers import (AdamW, AutoConfig, AutoTokenizer, get_linear_schedule_with_warmup, BertTokenizer, BertModel, BertConfig)\n","from data.processors.coqa import Extract_Features, Processor, Result\n","from data.processors.metrics import get_predictions\n","from data.processors.evaluate import CoQAEvaluator, parse_args\n","from transformers import BertModel, BertPreTrainedModel\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import CrossEntropyLoss\n","import csv\n","import numpy as np\n","\n","import evaluation\n","import argparse\n","import qrdatasets\n","import models\n","from utils import *\n","import random\n","import t5small\n","import t5base\n","import nltk\n","\n","import config\n","import json\n","\n","# CoQA dataset file\n","train_file=\"coqa-train-v1.0.json\"\n","predict_file=\"coqa-dev-v1.0.json\"\n","cur_path = os.getcwd()\n","output_directory = cur_path + \"/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/Bert_models\"\n","input_dir = cur_path + \"/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data\"\n","# can use either BERT base or BERT large\n","pretrained_model=\"bert-base-uncased\"\n","# pretrained_model=\"bert-large-uncased\"\n","# it's better to fine-tune Bert-base for 4 epoches than only one\n","epochs = 4\n","evaluation_batch_size=1\n","train_batch_size=2"]},{"cell_type":"markdown","metadata":{"id":"jMtRf2fppmO_"},"source":["## Classes and Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gow5D7uICeEX"},"outputs":[],"source":["#   our model is adapted from the baseline model of https://arxiv.org/pdf/1909.10772.pdf\n","\n","class BertBaseUncasedModel(BertPreTrainedModel):\n","\n","    #   Initialize Layers for our model\n","    def __init__(self,config,activation='relu'):\n","        super(BertBaseUncasedModel, self).__init__(config)\n","        self.bert = BertModel(config)\n","        hidden_size = config.hidden_size\n","        self.fc=nn.Linear(hidden_size,hidden_size)\n","        self.linear1 =nn.Linear(hidden_size,1)\n","        self.linear2= nn.Linear(hidden_size,2)\n","        self.activation = getattr(F, activation)\n","        self.init_weights()\n","\n","    def forward(self,input_ids,token_type_ids=None,attention_mask=None,start_positions=None,end_positions=None,rational_mask=None,cls_idx=None,head_mask=None):\n","        #   Bert-base outputs\n","        outputs = self.bert(input_ids,token_type_ids=token_type_ids,attention_mask=attention_mask,head_mask=head_mask)\n","        output_vector, bert_pooled_output = outputs\n","\n","        #   rational logits (rationale probability to calculate start and end logits)\n","        #   fc = w2 x relu(W1 x h)\n","        rational_logits = self.fc(output_vector)\n","        rational_logits = self.activation(self.linear1(rational_logits))\n","\n","        #   pr = sigmoid(fc)\n","        rational_logits = torch.sigmoid(rational_logits)\n","        #   h1 = pr x outputvector-h\n","        output_vector = output_vector * rational_logits\n","        mask = token_type_ids.type(output_vector.dtype)\n","        rational_logits = rational_logits.squeeze(-1) * mask\n","\n","        #   calculating start and end logits using FC(h1)\n","        start_end_logits = self.fc(output_vector)\n","        start_end_logits = self.activation(self.linear2(start_end_logits))\n","        \n","        start_logits, end_logits = start_end_logits.split(1, dim=-1)\n","        start_logits, end_logits = start_logits.squeeze(-1), end_logits.squeeze(-1)\n","        start_logits= start_logits * rational_logits\n","        end_logits =  end_logits * rational_logits\n","\n","        #   fc2 = wa2 x relu(Wa1 x h1)\n","        attention  = self.fc(output_vector)\n","        attention  = (self.activation(self.linear1(attention))).squeeze(-1)\n","\n","        #   a = SoftMax(fc2)\n","        attention = F.softmax(attention, dim=-1)\n","        attention_pooled_output = (attention.unsqueeze(-1) * output_vector).sum(dim=-2)\n","        unk_logits = self.fc(bert_pooled_output)\n","        unk_logits = self.activation(self.linear1(unk_logits))\n","\n","        #   calculate yes and no logits using pooled-output = FC(a)\n","        yes_no_logits =self.fc(attention_pooled_output)\n","        yes_no_logits =self.activation(self.linear2(yes_no_logits))\n","        yes_logits, no_logits = yes_no_logits.split(1, dim=-1)\n","\n","        if start_positions != None and end_positions != None:\n","            start_positions, end_positions = start_positions + cls_idx, end_positions + cls_idx\n","            start = torch.cat((yes_logits, no_logits, unk_logits, start_logits), dim=-1)\n","            end = torch.cat((yes_logits, no_logits, unk_logits, end_logits), dim=-1)\n","            if len(start_positions.size()) > 1:\n","                start_positions = start_positions.squeeze(-1)\n","            if len(end_positions.size()) > 1:\n","                end_positions = end_positions.squeeze(-1)\n","\n","            #   calculate cross entropy loss for start and end logits\n","            Entropy_loss = CrossEntropyLoss()\n","            start_loss = Entropy_loss(start, start_positions)\n","            end_loss = Entropy_loss(end, end_positions)\n","            #   Training objective: to minimize the total loss of both start and end logits\n","            total_loss = (start_loss + end_loss) / 2 \n","            return total_loss\n","        return start_logits, end_logits, yes_logits, no_logits, unk_logits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XlUwKuHbCeEg"},"outputs":[],"source":["### load dataset for training or evaluation\n","\n","def load_dataset(tokenizer, evaluate=True, cache_file_name=None, predict_file_name=None):\n","    '''\n","    converting raw coqa dataset into features to be processed by BERT  \n","    '''\n","    # print(os.path.join(input_dir,\"bert-base-uncased_train\"))\n","    # use user-defined cache_file_name or the default ones\n","    if cache_file_name is not None:\n","        cache_file = os.path.join(input_dir,cache_file_name)\n","    else:\n","        if evaluate:\n","            cache_file = os.path.join(input_dir,\"bert-base-uncased_dev\")\n","        else:\n","            cache_file = os.path.join(input_dir,\"bert-base-uncased_train\")\n","\n","    if os.path.exists(cache_file):\n","        print(\"Loading cache\",cache_file)\n","        features_and_dataset = torch.load(cache_file)\n","        features, dataset, examples = (\n","            features_and_dataset[\"features\"],features_and_dataset[\"dataset\"],features_and_dataset[\"examples\"])\n","    else:\n","        print(\"Creating features from dataset file at\", input_dir)\n","        if predict_file_name is not None:\n","            predict_file = predict_file_name\n","\n","        if not \"data\" and ((evaluate and not predict_file) or (not evaluate and not train_file)):\n","            raise ValueError(\"predict_file or train_file not found\")\n","        else:\n","            processor = Processor()\n","            if evaluate:\n","                # process the raw data, load only two historical conversation\n","                # def get_examples(self, data_dir, history_len, filename=None, threads=1)\n","                examples = processor.get_examples(input_dir, 2, filename=predict_file, threads=1)\n","            else:\n","                # process the raw data\n","                # def get_examples(self, data_dir, history_len, filename=None, threads=1)\n","                # number of examples is the same as the number of the QA pairs: 108647\n","                # each example is consist of question_text with 2 historical turn and the text, and ground truth start and end positions\n","                examples = processor.get_examples(input_dir, 2, filename=train_file, threads=1)\n","        \n","        # max_seq_length is the total length for input sequence of BERT \n","        features, dataset = Extract_Features(examples=examples,tokenizer=tokenizer,max_seq_length=512, doc_stride=128, max_query_length=64, is_training=not evaluate, threads=1)\n","    #   caching it in a cache file to reduce time\n","        torch.save({\"features\": features, \"dataset\": dataset, \"examples\": examples}, cache_file)\n","    if evaluate:\n","        return dataset, examples, features\n","    return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZyM0goV04hKB"},"outputs":[],"source":["### Load two fine-tuned models, but with diffetent load methods\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# load bert\n","bert_path = './mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/Bert_models/Bert_from_original_Surya_epoch4'\n","bert_model = BertBaseUncasedModel.from_pretrained(bert_path) \n","bert_tokenizer = BertTokenizer.from_pretrained(bert_path, do_lower_case=True)\n","bert_model.to(device)\n","\n","# load t5\n","t5_path = './mydata/MyDrive/CSNLP_Project/T5_model_COQAR/trained_models/t5_small_with_story_batch16_hist_3_mixed/epoch10'\n","# t5_path = './mydata/MyDrive/CSNLP_Project/T5_model_COQAR/trained_models/t5_base_with_story_batch4_hist_20/epoch6.zip'\n","\n","t5_model = torch.load(t5_path)\n","# t5_model = t5small.load_fine_tuned_model(t5_path)\n","t5_input_tokenizer = t5small.get_input_tokenizer()\n","t5_output_tokenizer = t5small.get_output_tokenizer()\n","t5_model.to(device)\n","\n","pass"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":333,"status":"ok","timestamp":1655564631239,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"EVlsaahMN_EZ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4990040b-a174-43a0-98b5-9d4a6f050d93"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":8}],"source":["# default parameters for T5\n","hparams = {\n","    'epochs' : 3,\n","    'learning_rate' : 0.00005,\n","    'batch_size' : 16,\n","    'weight_decay' : 0.0,\n","    'history_size' : 3,\n","    'dropout_rate' : 0.1,\n","    'include_story' : True,\n","    'model_size' : 'small'\n","}\n","nltk.download('wordnet')"]},{"cell_type":"code","source":["# get rewritten answers from T5\n","\n","def generate_new_dataset(input_dir,file_type='train',t5_type='base',t5_model=t5_model,method='append'):\n","### open the COQA dataset\n","    if file_type == 'train':\n","        # training dataset\n","        with open(os.path.join(input_dir, train_file), \"r\", encoding=\"utf-8\") as reader:\n","            input_data = json.load(reader)\n","    elif file_type == 'dev':\n","        # dev dataset\n","        with open(os.path.join(input_dir, predict_file), \"r\", encoding=\"utf-8\") as reader:\n","            input_data = json.load(reader)\n","    else:\n","        raise Exception(\"must specify a file type: train/dev\")\n","        pass\n","\n","    # create empty dictionary\n","    t5_data = {'input':[], 'references':[], 'context':[]}\n","\n","    # create dataset for T5\n","    for input_d in input_data[\"data\"]:\n","        questions = []\n","        assert len(input_d['questions']) == len(input_d['answers'])\n","        for id_q in range(len(input_d['questions'])):\n","            if id_q == 0:\n","                questions.append(input_d['questions'][id_q]['input_text'])\n","            else:\n","                questions.append(input_d['answers'][id_q-1]['input_text'])\n","                questions.append(input_d['questions'][id_q]['input_text'])\n","            t5_data['input'].append(questions.copy())\n","            t5_data['references'].append([''])\n","            t5_data['context'].append(input_d['story'])\n","\n","    # T5 make dataset\n","    if t5_type == 'base':\n","        dataset = t5base.make_dataset(t5_data, hparams, cuda = True)\n","    elif t5_type == 'small':\n","        dataset = t5base.make_dataset(t5_data, hparams, cuda = True)\n","    else:\n","        raise Exception(\"must specify a t5 model type: base/small\")\n","        pass\n","\n","    # generate rewritten questions\n","    loader = DataLoader(dataset=dataset, batch_size=hparams['batch_size'])\n","    t5_model.cuda()\n","    t5_model.train(False)\n","    rewritten_qs = []\n","    for dic in loader:\n","        output = t5_model.generate(input_ids = dic['input_ids'], attention_mask = dic['attention_mask'])\n","        pred = t5_output_tokenizer.batch_decode(output, skip_special_tokens = True)\n","        rewritten_qs += pred\n","\n","    ### Create new training/dev dataset json file for Bert\n","    # append or replace with the generated question \n","    count = 0\n","    for story_id in range(len(input_data[\"data\"])):\n","        for question_id in range(len(input_data[\"data\"][story_id]['questions'])):\n","            if method == 'append':\n","                # append the rewritten question\n","                input_data[\"data\"][story_id]['questions'][question_id]['input_text'] += (' '+rewritten_qs[count])\n","            elif method == 'replace':\n","                # replace with the rewritten question\n","                input_data[\"data\"][story_id]['questions'][question_id]['input_text'] = rewritten_qs[count]\n","            else:\n","                raise Exception(\"must specify a method: append/replace\")\n","                pass\n","            count += 1\n","\n","    # save as new dataset json file\n","    file_name = 'coqa-{}-v1.0-{}_with_T5.json'.format(file_type,method)\n","    # with open(os.path.join( input_dir, 'coqa-dev-v1.0-with_T5.json'), 'w', encoding=\"utf-8\") as outfile:\n","    with open(os.path.join(input_dir, file_name), 'w', encoding=\"utf-8\") as outfile:\n","        json.dump(input_data, outfile)\n","\n","\n","generate_new_dataset(input_dir,file_type='dev',t5_type='small',t5_model=t5_model,method='replace')"],"metadata":{"id":"fzxnuGYN1cJl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655564240466,"user_tz":-120,"elapsed":286927,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"}},"outputId":"9a9e21c8-9ad7-481e-a61b-5f80cfbe1d15"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (539 > 512). Running this sequence through the model will result in indexing errors\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BwlJfybiUMvt"},"outputs":[],"source":["### wrtiting predictions with fine-tuned model\n","\n","def convert_to_list(tensor):\n","    return tensor.detach().cpu().tolist()\n","\n","\n","def Write_predictions(model, tokenizer, device, variant_name,cache_file_name=None,predict_file_name=None):\n","    # generate catch file processed from the json dataset\n","    dataset, examples, features = load_dataset(tokenizer, evaluate=True, cache_file_name=cache_file_name, predict_file_name=predict_file_name)\n","    \n","    if not os.path.exists(output_directory+'/'+variant_name):\n","        os.makedirs(output_directory+'/'+variant_name)\n","        \n","    #   wrtiting predictions once training is complete\n","    evalutation_sampler = SequentialSampler(dataset)\n","    evaluation_dataloader = DataLoader(dataset, sampler=evalutation_sampler, batch_size=evaluation_batch_size)\n","    mod_results = []\n","    for batch in tqdm(evaluation_dataloader, desc=\"Evaluating\"):\n","        model.eval()\n","        batch = tuple(t.to(device) for t in batch)\n","        with torch.no_grad():\n","            # each batch has 4 elements, the last is the examle_indeces\n","            inputs = {\"input_ids\": batch[0],\"token_type_ids\": batch[1],\"attention_mask\": batch[2]}\n","            # indices of ConvQA example in this batch\n","            example_indices = batch[3]\n","            outputs = model(**inputs)\n","        for i, example_index in enumerate(example_indices):\n","            eval_feature = features[example_index.item()]\n","            unique_id = int(eval_feature.unique_id)\n","            output = [convert_to_list(output[i]) for output in outputs]\n","            start_logits, end_logits, yes_logits, no_logits, unk_logits = output\n","            result = Result(unique_id=unique_id, start_logits=start_logits, end_logits=end_logits, yes_logits=yes_logits, no_logits=no_logits, unk_logits=unk_logits)\n","            mod_results.append(result)\n","\n","    # Get predictions for development dataset and store it in predictions.json\n","    output_prediction_file = os.path.join(output_directory+'/'+variant_name, \"predictions.json\")\n","    get_predictions(examples, features, mod_results, 20, 30, True, output_prediction_file, False, tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"XDyYsqyW9JEY"},"source":["## Prediction\n","\n","predict on dev dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f-lZDiEb4OcD","executionInfo":{"status":"ok","timestamp":1655565530990,"user_tz":-120,"elapsed":890222,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"}},"outputId":"3fd81037-1379-46a9-9311-e7d5f21b2dec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Bert_from_original_Surya_epoch4\n","Creating features from dataset file at /content/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data\n"]},{"output_type":"stream","name":"stderr","text":["Extracting features from dataset: 100%|██████████| 500/500 [05:44<00:00,  1.45it/s]\n","Extracting features from dataset: 100%|██████████| 7983/7983 [02:58<00:00, 44.78it/s]\n","Tag unique id to each example: 100%|██████████| 7983/7983 [00:00<00:00, 478289.42it/s]\n","Evaluating: 100%|██████████| 8635/8635 [05:19<00:00, 27.04it/s]\n","Writing preditions: 100%|██████████| 7983/7983 [00:28<00:00, 276.85it/s]\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model_parameter_directory = [ f.path for f in os.scandir(output_directory) if f.is_dir() ]\n","\n","# use catch file name\n","cache_file_name = 'bert-base-uncased_dev_with_T5'\n","# use the predict file name\n","predict_file_name = 'coqa-dev-v1.0-with_T5.json'\n","# # reset the catch file name\n","# cache_file_name = None\n","# # reset the predict file name\n","# predict_file_name = None\n","\n","# for m in model_parameter_directory:\n","m = model_parameter_directory[0]\n","variant_name = m.split('/')[-1]\n","print(variant_name)\n","# m = m + '/pytorch_model_2.bin'\n","model = BertBaseUncasedModel.from_pretrained(m) \n","tokenizer = BertTokenizer.from_pretrained(m, do_lower_case=True)\n","model.to(device)\n","Write_predictions(model, tokenizer, device, variant_name, cache_file_name, predict_file_name)"]},{"cell_type":"markdown","metadata":{"id":"L_AFYP9k0Yu1"},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4VY9u1rP0cSu"},"outputs":[],"source":["evaluator = CoQAEvaluator(input_dir+'/'+predict_file)\n","\n","variant_name = 'Bert_from_original_Surya_epoch4'\n","\n","pre_file_bert = output_directory+'/'+variant_name+'/'+'predictions.json'\n","\n","# evaluate\n","with open(pre_file_bert) as f:\n","    pred_data = CoQAEvaluator.preds_to_dict(pre_file_bert)\n","\n","# write evaluate results\n","with open(output_directory+'/'+variant_name+'/'+'evaluation.json', 'w') as f:\n","    json.dump(evaluator.model_performance(pred_data), f, indent=2)\n","\n","# show\n","print(json.dumps(evaluator.model_performance(pred_data), indent=2))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"BertWithT5.ipynb","provenance":[]},"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"},"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}