{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14798,"status":"ok","timestamp":1657026581648,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"JfgX6lC-CeEN","outputId":"fe01dfe9-9fa0-4093-d948-f516f69f797c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers==4.19.2 in /usr/local/lib/python3.7/dist-packages (4.19.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.2) (21.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.2) (0.8.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.2) (0.12.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.2) (4.11.4)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.2) (4.64.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.2) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.2) (2.23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.2) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.2) (3.7.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.19.2) (1.21.6)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.19.2) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.19.2) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.19.2) (3.8.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.2) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.2) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.2) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.19.2) (2022.6.15)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: rouge_metric in /usr/local/lib/python3.7/dist-packages (1.0.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk==3.6.5 in /usr/local/lib/python3.7/dist-packages (3.6.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.5) (4.64.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.5) (2022.6.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.5) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.5) (7.1.2)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n","Drive already mounted at ./mydata; to attempt to forcibly remount, call drive.mount(\"./mydata\", force_remount=True).\n"]}],"source":["!pip install transformers==4.19.2\n","!pip install rouge_metric\n","!pip install nltk==3.6.5\n","!pip install sentencepiece\n","from google.colab import drive\n","drive.mount('./mydata')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"wyzGkzsuHNAA","executionInfo":{"status":"ok","timestamp":1657027567125,"user_tz":-120,"elapsed":229,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"}}},"outputs":[],"source":["import sys\n","sys.path.append('./mydata/MyDrive/CSNLP_Project/Bert_model_COQA')\n","sys.path.append('./mydata/MyDrive/CSNLP_Project/T5_model_COQAR')\n","sys.path.append('./mydata/MyDrive/CSNLP_Project/T5_model_COQAR/rewriting')"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"UZAJcRiiEZqp","executionInfo":{"status":"ok","timestamp":1657027582677,"user_tz":-120,"elapsed":14689,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"}}},"outputs":[],"source":["import collections\n","import glob\n","import os\n","import torch\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from tqdm import tqdm, trange\n","from transformers import (AdamW, AutoConfig, AutoTokenizer, get_linear_schedule_with_warmup, BertTokenizer, BertModel, BertConfig)\n","from processors.coqa import Extract_Features, Processor, Result\n","from processors.evaluate import CoQAEvaluator, parse_args\n","from processors.Bert_model import BertBaseUncasedModel, BertBaseUncasedModel_with_T5, load_dataset, Write_predictions, Write_predictions_with_T5\n","\n","#from transformers import BertModel, BertPreTrainedModel\n","\n","import csv\n","import numpy as np\n","\n","import evaluation\n","import argparse\n","import qrdatasets\n","import models\n","from utils import *\n","import random\n","import t5small\n","import t5base\n","import nltk\n","\n","import config\n","import json\n","\n","# CoQA dataset file\n","train_file=\"coqa-train-v1.0.json\"\n","predict_file=\"coqa-dev-v1.0.json\"\n","cur_path = os.getcwd()\n","output_directory = cur_path + \"/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/Bert_models\"\n","input_dir = cur_path + \"/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data\"\n","# can use either BERT base or BERT large\n","pretrained_model=\"bert-base-uncased\"\n","# pretrained_model=\"bert-large-uncased\"\n","# it's better to fine-tune Bert-base for 4 epoches than only one\n","epochs = 2\n","evaluation_batch_size=1\n","train_batch_size=2"]},{"cell_type":"markdown","metadata":{"id":"jvCeeKjL1PBp"},"source":["### create now dataset (json file)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZyM0goV04hKB"},"outputs":[],"source":["### Load two fine-tuned models, but with diffetent load methods\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# load bert\n","# bert_path = './mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/Bert_models/Bert_from_original_Surya_epoch4'\n","# bert_model = BertBaseUncasedModel.from_pretrained(bert_path) \n","# bert_tokenizer = BertTokenizer.from_pretrained(bert_path, do_lower_case=True)\n","# bert_model.to(device)\n","\n","# load t5\n","# t5_path = './mydata/MyDrive/CSNLP_Project/T5_model_COQAR/trained_models/t5_small_with_story_batch16_hist_3_mixed/epoch10'\n","t5_path = './mydata/MyDrive/CSNLP_Project/T5_model_COQAR/trained_models/t5_base_with_story_batch4_hist_20/epoch6.zip'\n","\n","t5_model = torch.load(t5_path)\n","# t5_input_tokenizer = t5small.get_input_tokenizer()\n","# t5_output_tokenizer = t5small.get_output_tokenizer()\n","t5_input_tokenizer = t5base.get_input_tokenizer()\n","t5_output_tokenizer = t5base.get_output_tokenizer()\n","t5_model.to(device)\n","\n","pass"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1657027582678,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"EVlsaahMN_EZ","outputId":"c3ef7521-798e-413e-f575-cd5c5aeed46b"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}],"source":["# default parameters for T5\n","hparams = {\n","    'epochs' : 2,\n","    'learning_rate' : 0.00005,\n","    'batch_size' : 4,\n","    'weight_decay' : 0.0,\n","    'history_size' : 3,\n","    'dropout_rate' : 0.1,\n","    'include_story' : True,\n","    'model_size' : 'small'\n","}\n","nltk.download('wordnet')"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"cSwK3c3Rp3uo","executionInfo":{"status":"ok","timestamp":1657027582679,"user_tz":-120,"elapsed":17,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"}}},"outputs":[],"source":["def make_t5_dataset(input_dir,file_type='train',t5_type='base',train_file=train_file):\n","    ### open the COQA dataset\n","    if file_type == 'train':\n","        # training dataset\n","        with open(os.path.join(input_dir, train_file), \"r\", encoding=\"utf-8\") as reader:\n","            input_data = json.load(reader)\n","    elif file_type == 'dev':\n","        # dev dataset\n","        with open(os.path.join(input_dir, predict_file), \"r\", encoding=\"utf-8\") as reader:\n","            input_data = json.load(reader)\n","    else:\n","        raise Exception(\"must specify a file type: train/dev\")\n","        pass\n","    \n","    # create empty dictionary\n","    t5_data = {'input':[], 'references':[], 'context':[]}\n","    # create dataset for T5\n","    for input_d in input_data[\"data\"]:\n","        questions = []\n","        assert len(input_d['questions']) == len(input_d['answers'])\n","        for id_q in range(len(input_d['questions'])):\n","            curr_question = input_d['questions'][id_q]['input_text']\n","            if '?' not in curr_question:\n","                if len(curr_question) == 0:\n","                    pass\n","                # some question in statement form end with '.'\n","                elif curr_question[-1] in ['.']:\n","                    if curr_question[-2:] == '/.':\n","                        curr_question = curr_question[:-2] + '?'\n","                    elif ('True or False' in curr_question) or ('Name' in curr_question):\n","                        pass\n","                    else:\n","                        # not worth to deal with such small portion of data\n","                        pass\n","                elif curr_question[-1] in ['/']:\n","                    curr_question = curr_question[:-1] + '?'\n","                else:\n","                    curr_question += '?'\n","            if id_q == 0:\n","                questions.append(curr_question)\n","            else:\n","                questions.append(input_d['answers'][id_q-1]['input_text'])\n","                questions.append(curr_question)\n","            t5_data['input'].append(questions.copy())\n","            t5_data['references'].append([''])\n","            t5_data['context'].append(input_d['story'])\n","\n","    # T5 make dataset\n","    if t5_type == 'base':\n","        dataset = t5base.make_dataset(t5_data, hparams, cuda = True)\n","    elif t5_type == 'small':\n","        dataset = t5base.make_dataset(t5_data, hparams, cuda = True)\n","    else:\n","        raise Exception(\"must specify a t5 model type: base/small\")\n","    \n","    return dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1o7NxaOY6VcT"},"outputs":[],"source":["# get rewritten answers from T5\n","\n","def generate_new_dataset(input_dir,file_type='train',t5_type='base',t5_model=t5_model,method='append'):\n","    ### open the COQA dataset\n","    if file_type == 'train':\n","        # training dataset\n","        with open(os.path.join(input_dir, train_file), \"r\", encoding=\"utf-8\") as reader:\n","            input_data = json.load(reader)\n","    elif file_type == 'dev':\n","        # dev dataset\n","        with open(os.path.join(input_dir, predict_file), \"r\", encoding=\"utf-8\") as reader:\n","            input_data = json.load(reader)\n","    else:\n","        raise Exception(\"must specify a file type: train/dev\")\n","        pass\n","        \n","    if 'repeat' not in method:\n","        # get dataset\n","        dataset = make_t5_dataset(input_dir,file_type=file_type,t5_type=t5_type,train_file=train_file)\n","\n","        ### generate rewritten questions\n","        loader = DataLoader(dataset=dataset, batch_size=hparams['batch_size'])\n","        t5_model.cuda()\n","        t5_model.train(False)\n","        rewritten_qs = []\n","        for dic in tqdm(loader, desc=\"Generating rewritten questions\"):\n","            output = t5_model.generate(input_ids = dic['input_ids'], attention_mask = dic['attention_mask'])\n","            pred = t5_output_tokenizer.batch_decode(output, skip_special_tokens = True)\n","            rewritten_qs += pred\n","\n","    ### Create new training/dev dataset json file for Bert\n","    # append or replace with the generated question \n","    count = 0\n","    for story_id in range(len(input_data[\"data\"])):\n","        for question_id in range(len(input_data[\"data\"][story_id]['questions'])):\n","            # if the rewritten question from T5 is failed, we use the original question.\n","            # also clean the question data\n","            curr_question = input_data[\"data\"][story_id]['questions'][question_id]['input_text']\n","            if '?' not in curr_question:\n","                if len(curr_question) == 0:\n","                    pass\n","                # some question in statement form end with '.'\n","                elif curr_question[-1] in ['.']:\n","                    if curr_question[-2:] == '/.':\n","                        curr_question = curr_question[:-2] + '?'\n","                    elif ('True or False' in curr_question) or ('Name' in curr_question):\n","                        pass\n","                    else:\n","                        # not worth to deal with such small portion of data\n","                        pass\n","                elif curr_question[-1] in ['/']:\n","                    curr_question = curr_question[:-1] + '?'\n","                else:\n","                    curr_question += '?'\n","\n","            if 'repeat' not in method:\n","                if len(rewritten_qs[count]) < 2:\n","                    pass\n","                # for failed rewritten questions(those not even questions)\n","                elif rewritten_qs[count][-1] != '?':\n","                    rewritten_qs[count] = curr_question\n","            if 'append' in method:\n","                # append the rewritten question\n","                input_data[\"data\"][story_id]['questions'][question_id]['input_text'] = curr_question + (' '+rewritten_qs[count])\n","            elif 'replace' in method:\n","                # replace with the rewritten question\n","                input_data[\"data\"][story_id]['questions'][question_id]['input_text'] = rewritten_qs[count]\n","            elif 'repeat' in method:\n","                # repeat the curent question twice for comparison\n","                input_data[\"data\"][story_id]['questions'][question_id]['input_text'] = curr_question + (' '+curr_question)\n","            else:\n","                raise Exception(\"must specify a method: append/replace\")\n","                pass\n","            count += 1\n","\n","    # save as new dataset json file\n","    file_name = 'coqa-{}-v1.0-{}_with_T5.json'.format(file_type,method)\n","    # with open(os.path.join( input_dir, 'coqa-dev-v1.0-with_T5.json'), 'w', encoding=\"utf-8\") as outfile:\n","    with open(os.path.join(input_dir, file_name), 'w', encoding=\"utf-8\") as outfile:\n","        json.dump(input_data, outfile)\n","\n","generate_new_dataset(input_dir,file_type='dev',t5_type='base',t5_model=t5_model,method='append_v3')"]},{"cell_type":"markdown","metadata":{"id":"fFtnkFKgvWXh"},"source":["### train bert with t5 embedding"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"lymNPhfWl7-E","executionInfo":{"status":"ok","timestamp":1657027583015,"user_tz":-120,"elapsed":352,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"}}},"outputs":[],"source":["def train_bert_with_t5(bert_model, t5_model, bert_tokenizer, bert_dataset, t5_dataset, device, pretrained_model, epochs_trained=0, start_iter=0, start_total_loss=0, batch_size=2, save_criteria=None):\n","    save_flag = True\n","\n","    bert_train_sampler = SequentialSampler(bert_dataset) \n","    bert_dataloader = DataLoader(bert_dataset, sampler=bert_train_sampler, batch_size=batch_size)\n","    t5_dataloader = DataLoader(dataset=t5_dataset, batch_size=batch_size)\n","    t_total = len(bert_dataloader) // 1 * epochs\n","\n","    # Preparing optimizer and scheduler\n","    optimizer_parameters = [{\"params\": [p for n, p in bert_model.named_parameters() if not any(nd in n for nd in [\"bias\", \"LayerNorm.weight\"])],\"weight_decay\": 0.01,},{\"params\": [p for n, p in bert_model.named_parameters() if any(nd in n for nd in [\"bias\", \"LayerNorm.weight\"])], \"weight_decay\": 0.0}]\n","    optimizer = AdamW(optimizer_parameters,lr=1e-5, eps=1e-8)\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=2000, num_training_steps=t_total)\n","\n","    # Check if saved optimizer or scheduler states exist\n","    if os.path.isfile(os.path.join(pretrained_model, \"optimizer.pt\")) and os.path.isfile(os.path.join(pretrained_model, \"scheduler.pt\")):\n","        optimizer.load_state_dict(torch.load(\n","            os.path.join(pretrained_model, \"optimizer.pt\")))\n","        scheduler.load_state_dict(torch.load(\n","            os.path.join(pretrained_model, \"scheduler.pt\")))\n","\n","    counter = 1\n","    # epochs_trained = 0\n","    train_loss, loss = start_total_loss, 0.0\n","    bert_model.zero_grad()\n","    iterator = trange(0, int(epochs), desc=\"Epoch\", disable=False)\n","    for epoch_id in iterator:\n","        epoch_iterator = tqdm(zip(bert_dataloader,t5_dataloader), desc=\"Iteration\", disable=True)\n","        for i,batch in enumerate(epoch_iterator):\n","            # skip the first few iterations\n","            if (epoch_id < epochs_trained) or ((epoch_id == epochs_trained) and (i<=start_iter)):\n","                counter += 1\n","                continue\n","            batch_bert, batch_t5 = batch\n","            # model.encoder(input_ids=s, attention_mask=attn, return_dict=True)\n","            # pooled_sentence = output.last_hidden_state # shape is [batch_size, seq_len, hidden_size]\n","            # # pooled_sentence will represent the embeddings for each word in the sentence\n","            # # you need to sum/average the pooled_sentence\n","            # pooled_sentence = torch.mean(pooled_sentence, dim=1)\n","            t5_embdeding = t5_model.encoder(input_ids = batch_t5['input_ids'], attention_mask = batch_t5['attention_mask'], return_dict=False)\n","            bert_model.train()\n","            batch_bert = tuple(t.to(device) for t in batch_bert)\n","            # inputs = { \"input_ids\": batch_bert[0],\"token_type_ids\": batch_bert[1], \"attention_mask\": batch_bert[2],\"start_positions\": batch_bert[3],\"end_positions\": batch_bert[4],\"rational_mask\": batch_bert[5],\"cls_idx\": batch_bert[6]}\n","            # loss = bert_model(**inputs)\n","            try:\n","                loss = bert_model(batch_bert[0],t5_embdeding,batch_t5,token_type_ids=batch_bert[1],attention_mask=batch_bert[2],start_positions=batch_bert[3],end_positions=batch_bert[4],rational_mask=batch_bert[5],cls_idx=batch_bert[6],head_mask=None)\n","            except:\n","                return batch_t5,t5_embdeding, bert_model.bert(batch_bert[0],token_type_ids=batch_bert[0],attention_mask=batch_bert[0],head_mask=None)[0]\n","                print(ts5_embdeding.size(),t5_embdeding)\n","                raise ValueError('Sizes of tensors must match except in dimension 1. Expected size 4 but got size 3 for tensor number 1 in the list.')\n","            loss.backward()\n","            train_loss += loss.item()\n","\n","            #   optimizing training parameters\n","            if (i + 1) % 1 == 0:\n","                optimizer.step()\n","                scheduler.step()  \n","                bert_model.zero_grad()\n","                counter += 1\n","                #   Saving model weights every 1000 iterations\n","                if counter % 1000 == 0:\n","                    output_dir = os.path.join(output_directory, \"model_weights\"+str(epochs_trained))\n","                    if not os.path.exists(output_dir):\n","                        os.makedirs(output_dir)\n","                    model_to_save = bert_model.module if hasattr(bert_model, \"module\") else bert_model\n","                    model_to_save.save_pretrained(output_dir)\n","                    bert_tokenizer.save_pretrained(output_dir)\n","                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n","                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n","            if (i+1) % 1000 == 0:\n","                print('iter: {}, loss: {}'.format(i,train_loss/counter))\n","\n","            if save_criteria and save_flag and ((train_loss/counter)<save_criteria):\n","                save_flag = False\n","                variant_name = 'Bert_with_T5_embdding_cond'\n","\n","                #   create output directory for model parameters and to write predictions\n","                if not os.path.exists(output_directory+'/'+variant_name) :\n","                    os.makedirs(output_directory+'/'+variant_name)\n","                            \n","                model_to_save = bert_model.module if hasattr(bert_model, \"module\") else bert_model\n","                model_to_save.save_pretrained(output_directory+'/'+variant_name)\n","                bert_tokenizer.save_pretrained(output_directory+'/'+variant_name)\n","                return train_loss/counter\n","\n","    return train_loss/counter"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y8kuWh347W1-","executionInfo":{"status":"ok","timestamp":1657059026580,"user_tz":-120,"elapsed":31441087,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"}},"outputId":"7c8d97c0-4215-4614-ee54-d5d1050098e9"},"outputs":[{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (526 > 512). Running this sequence through the model will result in indexing errors\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","Epoch:  50%|█████     | 2/4 [00:18<00:18,  9.04s/it]"]},{"output_type":"stream","name":"stdout","text":["iter: 999, loss: 1.718697674579905\n","iter: 1999, loss: 1.7154454343795387\n","iter: 2999, loss: 1.7120449411947056\n","iter: 3999, loss: 1.7078826507375595\n","iter: 4999, loss: 1.7041580147901363\n","iter: 5999, loss: 1.6997955380414198\n","iter: 6999, loss: 1.6956273222594866\n","iter: 7999, loss: 1.6918163549196867\n","iter: 8999, loss: 1.6875505029098827\n","iter: 9999, loss: 1.6839742908684898\n","iter: 10999, loss: 1.6796338340592505\n","iter: 11999, loss: 1.6758012112558638\n","iter: 12999, loss: 1.6708181254798895\n","iter: 13999, loss: 1.6664422750443995\n","iter: 14999, loss: 1.6623551913966608\n","iter: 15999, loss: 1.6588187036743112\n","iter: 16999, loss: 1.6548569129907174\n","iter: 17999, loss: 1.651341099172254\n","iter: 18999, loss: 1.647821336267711\n","iter: 19999, loss: 1.6448537143376398\n","iter: 20999, loss: 1.6413972444571185\n","iter: 21999, loss: 1.6383931495196424\n","iter: 22999, loss: 1.6351515803602292\n","iter: 23999, loss: 1.6324637828750006\n","iter: 24999, loss: 1.6295267137417513\n","iter: 25999, loss: 1.6263948949302605\n","iter: 26999, loss: 1.6231106316858843\n","iter: 27999, loss: 1.6197702149172146\n","iter: 28999, loss: 1.6176066618693163\n","iter: 29999, loss: 1.6148734815919867\n","iter: 30999, loss: 1.6121887697305797\n","iter: 31999, loss: 1.6087982190408976\n","iter: 32999, loss: 1.606349547126034\n","iter: 33999, loss: 1.6033306215195358\n","iter: 34999, loss: 1.6003257494883938\n","iter: 35999, loss: 1.5974111070501067\n","iter: 36999, loss: 1.5952954417567873\n","iter: 37999, loss: 1.5925222343702772\n","iter: 38999, loss: 1.5901256719767278\n","iter: 39999, loss: 1.587501739775947\n","iter: 40999, loss: 1.5859267686764453\n","iter: 41999, loss: 1.583673871902143\n","iter: 42999, loss: 1.581510128703401\n","iter: 43999, loss: 1.5799368800996851\n","iter: 44999, loss: 1.578279245483214\n","iter: 45999, loss: 1.5766451438796265\n","iter: 46999, loss: 1.5750139107233623\n","iter: 47999, loss: 1.5732138085328633\n","iter: 48999, loss: 1.5728574821497783\n","iter: 49999, loss: 1.5708680449172436\n","iter: 50999, loss: 1.5694968718797784\n","iter: 51999, loss: 1.5673803004175335\n","iter: 52999, loss: 1.5664319017184496\n","iter: 53999, loss: 1.5651620610952746\n","iter: 54999, loss: 1.5646102563942557\n","iter: 55999, loss: 1.563491106346645\n","iter: 56999, loss: 1.562775665875674\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:  75%|███████▌  | 3/4 [4:19:06<1:58:15, 7095.69s/it]"]},{"output_type":"stream","name":"stdout","text":["iter: 999, loss: 1.5590968305715172\n","iter: 1999, loss: 1.556243282054243\n","iter: 2999, loss: 1.5530982265544861\n","iter: 3999, loss: 1.5496324140768134\n","iter: 4999, loss: 1.5464749481805782\n","iter: 5999, loss: 1.5430859976411209\n","iter: 6999, loss: 1.5396316183790315\n","iter: 7999, loss: 1.536493700357619\n","iter: 8999, loss: 1.5330522259282429\n","iter: 9999, loss: 1.529956674733461\n","iter: 10999, loss: 1.5264374427805154\n","iter: 11999, loss: 1.5231673767503742\n","iter: 12999, loss: 1.5192982442639713\n","iter: 13999, loss: 1.5155293618270573\n","iter: 14999, loss: 1.51211650290441\n","iter: 15999, loss: 1.5090068488210553\n","iter: 16999, loss: 1.5056268128297323\n","iter: 17999, loss: 1.5026412739892907\n","iter: 18999, loss: 1.499580875158707\n","iter: 19999, loss: 1.4967385308179209\n","iter: 20999, loss: 1.4936402021558077\n","iter: 21999, loss: 1.491078171954485\n","iter: 22999, loss: 1.4883069986491098\n","iter: 23999, loss: 1.485851012063867\n","iter: 24999, loss: 1.483152074726127\n","iter: 25999, loss: 1.4805240758116962\n","iter: 26999, loss: 1.4776705747529437\n","iter: 27999, loss: 1.4748963770244705\n","iter: 28999, loss: 1.4725403288303898\n","iter: 29999, loss: 1.4700152031602745\n","iter: 30999, loss: 1.4675083690879738\n","iter: 31999, loss: 1.4645999627137276\n","iter: 32999, loss: 1.4623112545836465\n","iter: 33999, loss: 1.459611800293056\n","iter: 34999, loss: 1.4569538146051146\n","iter: 35999, loss: 1.45435983654971\n","iter: 36999, loss: 1.4522702629904236\n","iter: 37999, loss: 1.4498622205707095\n","iter: 38999, loss: 1.4476584332554212\n","iter: 39999, loss: 1.4453640787656037\n","iter: 40999, loss: 1.4436071056416624\n","iter: 41999, loss: 1.4415846186626124\n","iter: 42999, loss: 1.4396109835031272\n","iter: 43999, loss: 1.437871993337287\n","iter: 44999, loss: 1.436229603346244\n","iter: 45999, loss: 1.4346090269926666\n","iter: 46999, loss: 1.4329810638571479\n","iter: 47999, loss: 1.4312543173200454\n","iter: 48999, loss: 1.4305706978686323\n","iter: 49999, loss: 1.4287556104776418\n","iter: 50999, loss: 1.4272944122892635\n","iter: 51999, loss: 1.4254610152863652\n","iter: 52999, loss: 1.4243288480721157\n","iter: 53999, loss: 1.4230922291167922\n","iter: 54999, loss: 1.4223304657560474\n","iter: 55999, loss: 1.42122733734405\n","iter: 56999, loss: 1.420502701257761\n"]},{"output_type":"stream","name":"stderr","text":["Epoch: 100%|██████████| 4/4 [8:38:14<00:00, 7773.50s/it] \n"]},{"output_type":"execute_result","data":{"text/plain":["1.419524834864827"]},"metadata":{},"execution_count":8}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","method = 'append'\n","epochs = 4\n","start_iter = 0\n","epochs_trained = 2\n","start_total_loss = 1.740229015707179 * (56999*2)\n","save_criteria = None\n","\n","# pretrained_model = './mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/Bert_models/Bert_with_T5_embedding/'\n","pretrained_model = './mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/Bert_models/Bert_with_T5_embedding_epoch2_batch2/'\n","# pretrained_model=\"bert-base-uncased\"\n","\n","## load raw model\n","# t5 \n","t5_path = './mydata/MyDrive/CSNLP_Project/T5_model_COQAR/trained_models/t5_base_with_story_batch4_hist_20/epoch6.zip'\n","t5_model = torch.load(t5_path)\n","t5_input_tokenizer = t5base.get_input_tokenizer()   # or t5small \n","t5_output_tokenizer = t5base.get_output_tokenizer()\n","t5_model.to(device)\n","t5_model.train(False)\n","# bert\n","config = BertConfig.from_pretrained(pretrained_model, return_dict=False)\n","bert_tokenizer = BertTokenizer.from_pretrained(pretrained_model)\n","bert_model = BertBaseUncasedModel_with_T5.from_pretrained(pretrained_model, from_tf=bool(\".ckpt\" in pretrained_model), config=config,cache_dir=None,)\n","bert_model.to(device)\n","\n","## get datasets\n","t5_dataset = make_t5_dataset(input_dir,file_type='train',t5_type='base',train_file=train_file)\n","# cache_file_name = 'bert-base-uncased_train_with_T5_{}_v3.2'.format(method)\n","# train_file_name = 'coqa-train-v1.0-{}_v3_with_T5.json'.format(method)\n","cache_file_name = 'bert-base-uncased_train'\n","train_file_name = 'coqa-train-v1.0.json'\n","\n","if cache_file_name is not None:\n","    cache_file = os.path.join(input_dir,cache_file_name)\n","\n","if os.path.exists(cache_file):\n","    features_and_dataset = torch.load(cache_file)\n","    features, bert_dataset, examples = (\n","        features_and_dataset[\"features\"],features_and_dataset[\"dataset\"],features_and_dataset[\"examples\"])\n","else:\n","    bert_dataset = load_dataset(bert_tokenizer, input_dir=input_dir, evaluate=False, cache_file_name=cache_file_name, train_file_name=None, append_method='append')\n","    features_and_dataset = torch.load(cache_file)\n","    features, bert_dataset, examples = (\n","        features_and_dataset[\"features\"],features_and_dataset[\"dataset\"],features_and_dataset[\"examples\"])\n","\n","t5_dataset_new = []\n","for feature in features:\n","    t5_dataset_new.append(t5_dataset[feature.example_index])\n","t5_dataset = t5_dataset_new\n","del t5_dataset_new\n","del examples\n","del features\n","del features_and_dataset\n","\n","# train\n","train_bert_with_t5(bert_model, t5_model, bert_tokenizer, bert_dataset, t5_dataset, device, pretrained_model, epochs_trained=epochs_trained, start_iter=start_iter, start_total_loss=start_total_loss, batch_size=2, save_criteria=save_criteria)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"He_78G3YTZqI"},"outputs":[],"source":["\n","variant_name = 'Bert_with_T5_rewritten_epoch4_append_v3.2'\n","\n","#   create output directory for model parameters and to write predictions\n","if not os.path.exists(output_directory+'/'+variant_name) :\n","    os.makedirs(output_directory+'/'+variant_name)\n","            \n","model_to_save = bert_model.module if hasattr(bert_model, \"module\") else bert_model\n","model_to_save.save_pretrained(output_directory+'/'+variant_name)\n","bert_tokenizer.save_pretrained(output_directory+'/'+variant_name)"]},{"cell_type":"markdown","metadata":{"id":"XDyYsqyW9JEY"},"source":["## Prediction\n","\n","predict on dev dataset"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":235,"status":"ok","timestamp":1657060747254,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"f-lZDiEb4OcD","outputId":"7ba81bf4-9647-4b9f-ab51-0c0c1f05d503"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/Bert_models/Bert_with_T5_rewritten_epoch4_append_v3.2',\n"," '/content/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/Bert_models/Bert_from_original_Surya_epoch2',\n"," '/content/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/Bert_models/Bert_with_T5_embedding_epoch2_batch4',\n"," '/content/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/Bert_models/Bert_with_T5_embedding_epoch2_batch2',\n"," '/content/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/Bert_models/model_weights2']"]},"metadata":{},"execution_count":9}],"source":["### Load two fine-tuned models, but with diffetent load methods\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model_parameter_directory = [ f.path for f in os.scandir(output_directory) if f.is_dir() ]\n","\n","# method = '_replace_v2'\n","method = '_append_v3'\n","\n","# use catch file name\n","cache_file_name = 'bert-base-uncased_dev_with_T5{}'.format(method)\n","# use the predict file name\n","predict_file_name = 'coqa-dev-v1.0-{}_with_T5.json'.format(method[1:])\n","# # reset the catch file name\n","# cache_file_name = None\n","# # reset the predict file name\n","# predict_file_name = None\n","\n","model_parameter_directory"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1009388,"status":"ok","timestamp":1657061770202,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"quccxYdDyZNp","outputId":"aef0542e-8c00-489f-ea5f-fcc0b86e02d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["model_weights2\n"]},{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (539 > 512). Running this sequence through the model will result in indexing errors\n"]},{"output_type":"stream","name":"stdout","text":["Creating features from dataset file at /content/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data\n"]},{"output_type":"stream","name":"stderr","text":["Extracting features from dataset: 100%|██████████| 500/500 [05:53<00:00,  1.41it/s]\n","Extracting features from dataset: 100%|██████████| 7983/7983 [02:47<00:00, 47.67it/s]\n","Tag unique id to each example: 100%|██████████| 7983/7983 [00:00<00:00, 451888.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Loading cache /content/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/bert-base-uncased_dev_epoch4\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 8565it [06:34, 21.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["save prediction file at: /content/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/Bert_models/model_weights2/predictions.json\n"]},{"output_type":"stream","name":"stderr","text":["Writing preditions: 100%|██████████| 7983/7983 [00:30<00:00, 264.24it/s]\n"]}],"source":["## load raw model\n","# t5 \n","t5_path = './mydata/MyDrive/CSNLP_Project/T5_model_COQAR/trained_models/t5_base_with_story_batch4_hist_20/epoch6.zip'\n","t5_model = torch.load(t5_path)\n","t5_input_tokenizer = t5base.get_input_tokenizer()   # or t5small \n","t5_output_tokenizer = t5base.get_output_tokenizer()\n","t5_model.to(device)\n","t5_model.train(False)\n","\n","# for m in model_parameter_directory:\n","m = model_parameter_directory[4]\n","variant_name = m.split('/')[-1]\n","print(variant_name)\n","# m = m + '/pytorch_model_2.bin'\n","bert_model = BertBaseUncasedModel_with_T5.from_pretrained(m) \n","# bert_model = BertBaseUncasedModel.from_pretrained(m) \n","bert_tokenizer = BertTokenizer.from_pretrained(m, do_lower_case=True)\n","bert_model.to(device)\n","\n","## get datasets\n","t5_dataset = make_t5_dataset(input_dir,file_type='dev',t5_type='base',train_file=train_file)\n","cache_file_name = 'bert-base-uncased_dev_epoch4'\n","predict_file_name = 'coqa-dev-v1.0.json'\n","\n","bert_dataset = load_dataset(bert_tokenizer, input_dir=input_dir, evaluate=True, cache_file_name=cache_file_name, predict_file_name=None, append_method='original')\n","\n","if cache_file_name is not None:\n","    cache_file = os.path.join(input_dir,cache_file_name)\n","\n","if os.path.exists(cache_file):\n","    features_and_dataset = torch.load(cache_file)\n","    features, dataset, examples = (\n","        features_and_dataset[\"features\"],features_and_dataset[\"dataset\"],features_and_dataset[\"examples\"])\n","\n","t5_dataset_new = []\n","for feature in features:\n","    t5_dataset_new.append(t5_dataset[feature.example_index])\n","t5_dataset = t5_dataset_new\n","del t5_dataset_new\n","\n","# Write_predictions(model, tokenizer, device, variant_name, input_dir=input_dir, output_directory=output_directory, cache_file_name=cache_file_name, predict_file_name=predict_file_name, method=method, append_method='append')\n","Write_predictions_with_T5(bert_model, t5_model, bert_tokenizer, device, variant_name, t5_dataset, input_dir=input_dir,output_directory=output_directory,cache_file_name=cache_file_name,predict_file_name=predict_file_name,evaluation_batch_size=1,method='', append_method='original')"]},{"cell_type":"markdown","metadata":{"id":"L_AFYP9k0Yu1"},"source":["## Evaluation"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7335,"status":"ok","timestamp":1657061854268,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"4VY9u1rP0cSu","outputId":"6cca071c-8032-4f6b-ddc1-ad0eb8bb43a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","  \"children_stories\": {\n","    \"em\": 66.5,\n","    \"f1\": 76.4,\n","    \"turns\": 1425\n","  },\n","  \"literature\": {\n","    \"em\": 65.6,\n","    \"f1\": 74.7,\n","    \"turns\": 1630\n","  },\n","  \"mid-high_school\": {\n","    \"em\": 65.4,\n","    \"f1\": 75.8,\n","    \"turns\": 1653\n","  },\n","  \"news\": {\n","    \"em\": 67.4,\n","    \"f1\": 78.4,\n","    \"turns\": 1649\n","  },\n","  \"wikipedia\": {\n","    \"em\": 71.8,\n","    \"f1\": 80.8,\n","    \"turns\": 1626\n","  },\n","  \"reddit\": {\n","    \"em\": 0.0,\n","    \"f1\": 0.0,\n","    \"turns\": 0\n","  },\n","  \"science\": {\n","    \"em\": 0.0,\n","    \"f1\": 0.0,\n","    \"turns\": 0\n","  },\n","  \"in_domain\": {\n","    \"em\": 67.4,\n","    \"f1\": 77.2,\n","    \"turns\": 7983\n","  },\n","  \"out_domain\": {\n","    \"em\": 0.0,\n","    \"f1\": 0.0,\n","    \"turns\": 0\n","  },\n","  \"overall\": {\n","    \"em\": 67.4,\n","    \"f1\": 77.2,\n","    \"turns\": 7983\n","  }\n","}\n"]}],"source":["evaluator = CoQAEvaluator(input_dir+'/'+predict_file_name)\n","\n","# variant_name = 'Bert_with_T5_rewritten_epoch4_append_v2'\n","\n","# variant_name = 'Bert_from_original_Surya_epoch4'\n","\n","# variant_name = 'Bert_with_T5_rewritten_epoch4_replace'\n","\n","# m = model_parameter_directory[4]\n","# variant_name = m.split('/')[-1]\n","\n","pre_file_bert = output_directory+'/'+variant_name+'/'+'predictions{}.json'.format('')\n","\n","# evaluate\n","with open(pre_file_bert) as f:\n","    pred_data = CoQAEvaluator.preds_to_dict(pre_file_bert)\n","\n","# write evaluate results\n","with open(output_directory+'/'+variant_name+'/'+'evaluation{}.json'.format(method), 'w') as f:\n","    json.dump(evaluator.model_performance(pred_data), f, indent=2)\n","\n","# show\n","print(json.dumps(evaluator.model_performance(pred_data), indent=2))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"BertWithT5.ipynb","provenance":[]},"gpuClass":"standard","interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"},"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}