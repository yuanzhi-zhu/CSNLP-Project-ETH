{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"JfgX6lC-CeEN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655566486068,"user_tz":-120,"elapsed":29982,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"}},"outputId":"2f9ee06e-b1af-49ce-ece6-09d218947632"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.20.0-py3-none-any.whl (4.4 MB)\n","\u001b[K     |████████████████████████████████| 4.4 MB 4.3 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 76.8 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 61.9 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.0\n","Mounted at ./mydata\n"]}],"source":["!pip install transformers\n","from google.colab import drive\n","drive.mount('./mydata')"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"wyzGkzsuHNAA","executionInfo":{"status":"ok","timestamp":1655566691917,"user_tz":-120,"elapsed":337,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"}}},"outputs":[],"source":["import sys\n","sys.path.append('./mydata/MyDrive/CSNLP_Project/Bert_model_COQA')"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"UZAJcRiiEZqp","executionInfo":{"status":"ok","timestamp":1655567092428,"user_tz":-120,"elapsed":4,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"}}},"outputs":[],"source":["import collections\n","import glob\n","import os\n","import torch\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from tqdm import tqdm, trange\n","from transformers import (AdamW, AutoConfig, AutoTokenizer, get_linear_schedule_with_warmup, BertTokenizer, BertModel, BertConfig)\n","from data.processors.coqa import Extract_Features, Processor, Result\n","from data.processors.metrics import get_predictions\n","from data.processors.evaluate import CoQAEvaluator, parse_args\n","from transformers import BertModel, BertPreTrainedModel\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import CrossEntropyLoss\n","import csv\n","import numpy as np\n","\n","import json\n","\n","# locations\n","train_file=\"coqa-train-v1.0.json\"\n","predict_file=\"coqa-dev-v1.0.json\"\n","cur_path = os.getcwd()\n","output_directory = cur_path + \"/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/Bert_models\"\n","input_dir = cur_path + \"/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data\"\n","# can use either BERT base or BERT large\n","pretrained_model=\"bert-base-uncased\"\n","# pretrained_model=\"bert-large-uncased\"\n","epochs = 4\n","evaluation_batch_size=16\n","train_batch_size=2"]},{"cell_type":"markdown","metadata":{"id":"jMtRf2fppmO_"},"source":["## Classes and Functions"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"gow5D7uICeEX","executionInfo":{"status":"ok","timestamp":1655566716277,"user_tz":-120,"elapsed":333,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"}}},"outputs":[],"source":["#   our model is adapted from the baseline model of https://arxiv.org/pdf/1909.10772.pdf\n","\n","class BertBaseUncasedModel(BertPreTrainedModel):\n","\n","    #   Initialize Layers for our model\n","    def __init__(self,config,activation='relu'):\n","        super(BertBaseUncasedModel, self).__init__(config)\n","        self.bert = BertModel(config)\n","        hidden_size = config.hidden_size\n","        self.fc=nn.Linear(hidden_size,hidden_size)\n","        self.linear1 =nn.Linear(hidden_size,1)\n","        self.linear2= nn.Linear(hidden_size,2)\n","        self.activation = getattr(F, activation)\n","        self.init_weights()\n","\n","    def forward(self,input_ids,token_type_ids=None,attention_mask=None,start_positions=None,end_positions=None,rational_mask=None,cls_idx=None,head_mask=None):\n","\n","        #   Bert-base outputs\n","        outputs = self.bert(input_ids,token_type_ids=token_type_ids,attention_mask=attention_mask,head_mask=head_mask)\n","        output_vector, bert_pooled_output = outputs\n","\n","        #   rational logits (rationale probability to calculate start and end logits)\n","        #   fc = w2 x relu(W1 x h)\n","        rational_logits = self.fc(output_vector)\n","        rational_logits = self.activation(self.linear1(rational_logits))\n","\n","        #   pr = sigmoid(fc)\n","        rational_logits = torch.sigmoid(rational_logits)\n","        #   h1 = pr x outputvector-h\n","        output_vector = output_vector * rational_logits\n","        mask = token_type_ids.type(output_vector.dtype)\n","        rational_logits = rational_logits.squeeze(-1) * mask\n","\n","        #   calculating start and end logits using FC(h1)\n","        start_end_logits = self.fc(output_vector)\n","        start_end_logits = self.activation(self.linear2(start_end_logits))\n","        \n","        start_logits, end_logits = start_end_logits.split(1, dim=-1)\n","        start_logits, end_logits = start_logits.squeeze(-1), end_logits.squeeze(-1)\n","        start_logits= start_logits * rational_logits\n","        end_logits =  end_logits * rational_logits\n","\n","        #   fc2 = wa2 x relu(Wa1 x h1)\n","        attention  = self.fc(output_vector)\n","        attention  = (self.activation(self.linear1(attention))).squeeze(-1)\n","\n","        #   a = SoftMax(fc2)\n","        attention = F.softmax(attention, dim=-1)\n","        attention_pooled_output = (attention.unsqueeze(-1) * output_vector).sum(dim=-2)\n","        unk_logits = self.fc(bert_pooled_output)\n","        unk_logits = self.activation(self.linear1(unk_logits))\n","\n","        #   calculate yes and no logits using pooled-output = FC(a)\n","        yes_no_logits =self.fc(attention_pooled_output)\n","        yes_no_logits =self.activation(self.linear2(yes_no_logits))\n","        yes_logits, no_logits = yes_no_logits.split(1, dim=-1)\n","\n","        if start_positions != None and end_positions != None:\n","            start_positions, end_positions = start_positions + cls_idx, end_positions + cls_idx\n","            start = torch.cat((yes_logits, no_logits, unk_logits, start_logits), dim=-1)\n","            end = torch.cat((yes_logits, no_logits, unk_logits, end_logits), dim=-1)\n","            if len(start_positions.size()) > 1:\n","                start_positions = start_positions.squeeze(-1)\n","            if len(end_positions.size()) > 1:\n","                end_positions = end_positions.squeeze(-1)\n","\n","            #   calculate cross entropy loss for start and end logits\n","            Entropy_loss = CrossEntropyLoss()\n","            start_loss = Entropy_loss(start, start_positions)\n","            end_loss = Entropy_loss(end, end_positions)\n","            #   Training objective: to minimize the total loss of both start and end logits\n","            total_loss = (start_loss + end_loss) / 2 \n","            return total_loss\n","        return start_logits, end_logits, yes_logits, no_logits, unk_logits\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"XlUwKuHbCeEg","executionInfo":{"status":"ok","timestamp":1655566983394,"user_tz":-120,"elapsed":339,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"}}},"outputs":[],"source":["### load dataset for training or evaluation\n","\n","def load_dataset(tokenizer, evaluate=False, cache_file_name=None, train_file_name=None):\n","    '''\n","    converting raw coqa dataset into features to be processed by BERT  \n","    '''\n","    # print(os.path.join(input_dir,\"bert-base-uncased_train\"))\n","    # use user-defined cache_file_name or the default ones\n","    if cache_file_name is not None:\n","        cache_file = os.path.join(input_dir,cache_file_name)\n","    else:\n","        if evaluate:\n","            cache_file = os.path.join(input_dir,\"bert-base-uncased_dev\")\n","        else:\n","            cache_file = os.path.join(input_dir,\"bert-base-uncased_train\")\n","\n","    if os.path.exists(cache_file):\n","        print(\"Loading cache\",cache_file)\n","        features_and_dataset = torch.load(cache_file)\n","        features, dataset, examples = (\n","            features_and_dataset[\"features\"],features_and_dataset[\"dataset\"],features_and_dataset[\"examples\"])\n","    else:\n","        print(\"Creating features from dataset file at\", input_dir)\n","        if train_file_name is not None:\n","            train_file = train_file_name\n","\n","        if not \"data\" and ((evaluate and not predict_file) or (not evaluate and not train_file)):\n","            raise ValueError(\"predict_file or train_file not found\")\n","        else:\n","            processor = Processor()\n","            if evaluate:\n","                # process the raw data, load only two historical conversation\n","                # def get_examples(self, data_dir, history_len, filename=None, threads=1)\n","                examples = processor.get_examples(input_dir, 2, filename=predict_file, threads=1)\n","            else:\n","                # process the raw data\n","                # def get_examples(self, data_dir, history_len, filename=None, threads=1)\n","                # number of examples is the same as the number of the QA pairs: 108647\n","                # each example is consist of question_text with 2 historical turn and the text, and ground truth start and end positions\n","                examples = processor.get_examples(input_dir, 2, filename=train_file, threads=1)\n","        \n","        # max_seq_length is the total length for input sequence of BERT \n","        features, dataset = Extract_Features(examples=examples,tokenizer=tokenizer,max_seq_length=512, doc_stride=128, max_query_length=64, is_training=not evaluate, threads=1)\n","    #   caching it in a cache file to reduce time\n","        torch.save({\"features\": features, \"dataset\": dataset, \"examples\": examples}, cache_file)\n","    if evaluate:\n","        return dataset, examples, features\n","    return dataset"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"jx46LBgUCeEn","executionInfo":{"status":"ok","timestamp":1655566771847,"user_tz":-120,"elapsed":367,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"}}},"outputs":[],"source":["### train function\n","\n","def convert_to_list(tensor):\n","    return tensor.detach().cpu().tolist()\n","\n","def train(train_dataset, model, tokenizer, device):\n","\n","    train_sampler = RandomSampler(train_dataset) \n","    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=train_batch_size)\n","    t_total = len(train_dataloader) // 1 * epochs\n","\n","    # Preparing optimizer and scheduler\n","    \n","    optimizer_parameters = [{\"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in [\"bias\", \"LayerNorm.weight\"])],\"weight_decay\": 0.01,},{\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in [\"bias\", \"LayerNorm.weight\"])], \"weight_decay\": 0.0}]\n","    optimizer = AdamW(optimizer_parameters,lr=1e-5, eps=1e-8)\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=2000, num_training_steps=t_total)\n","\n","    # Check if saved optimizer or scheduler states exist\n","    if os.path.isfile(os.path.join(pretrained_model, \"optimizer.pt\")) and os.path.isfile(os.path.join(pretrained_model, \"scheduler.pt\")):\n","        optimizer.load_state_dict(torch.load(\n","            os.path.join(pretrained_model, \"optimizer.pt\")))\n","        scheduler.load_state_dict(torch.load(\n","            os.path.join(pretrained_model, \"scheduler.pt\")))\n","\n","    counter = 1\n","    epochs_trained = 0\n","    train_loss, loss = 0.0, 0.0\n","    model.zero_grad()\n","    iterator = trange(epochs_trained, int(epochs), desc=\"Epoch\", disable=False)\n","    for _ in iterator:\n","        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=True)\n","        for i,batch in enumerate(epoch_iterator):\n","            model.train()\n","            batch = tuple(t.to(device) for t in batch)\n","            inputs = { \"input_ids\": batch[0],\"token_type_ids\": batch[1], \"attention_mask\": batch[2],\"start_positions\": batch[3],\"end_positions\": batch[4],\"rational_mask\": batch[5],\"cls_idx\": batch[6]}\n","            # loss = model(**inputs, return_dict=False)\n","            loss = model(**inputs)\n","            loss.backward()\n","            train_loss += loss.item()\n","\n","            #   optimizing training parameters\n","            if (i + 1) % 1 == 0:\n","                optimizer.step()\n","                scheduler.step()  \n","                model.zero_grad()\n","                counter += 1\n","                #   Saving model weights every 1000 iterations\n","                if counter % 1000 == 0:\n","                    output_dir = os.path.join(output_directory, \"model_weights\"+str(epochs_trained))\n","                    if not os.path.exists(output_dir):\n","                        os.makedirs(output_dir)\n","                    model_to_save = model.module if hasattr(model, \"module\") else model\n","                    model_to_save.save_pretrained(output_dir)\n","                    tokenizer.save_pretrained(output_dir)\n","                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n","                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n","            if (i+1) % 1000 == 0:\n","                print('iter: {}, loss: {}'.format(i,train_loss/counter))\n","    return train_loss/counter"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"VHubgnHpCeEo","executionInfo":{"status":"ok","timestamp":1655566774246,"user_tz":-120,"elapsed":361,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"}}},"outputs":[],"source":["### wrtiting predictions\n","\n","def Write_predictions(model, tokenizer, device, variant_name):\n","    dataset, examples, features = load_dataset(tokenizer, evaluate=True)\n","\n","    if not os.path.exists(output_directory+'/'+variant_name):\n","        os.makedirs(output_directory+'/'+variant_name)\n","        \n","    #   wrtiting predictions once training is complete\n","    evalutation_sampler = SequentialSampler(dataset)\n","    evaluation_dataloader = DataLoader(dataset, sampler=evalutation_sampler, batch_size=evaluation_batch_size)\n","    mod_results = []\n","    for batch in tqdm(evaluation_dataloader, desc=\"Evaluating\"):\n","        model.eval()\n","        batch = tuple(t.to(device) for t in batch)\n","        with torch.no_grad():\n","            # each batch has 4 elements, the last is the examle_indeces\n","            inputs = {\"input_ids\": batch[0],\"token_type_ids\": batch[1],\"attention_mask\": batch[2]}\n","            # indices of ConvQA example in this batch\n","            example_indices = batch[3]\n","            outputs = model(**inputs)\n","        for i, example_index in enumerate(example_indices):\n","            eval_feature = features[example_index.item()]\n","            unique_id = int(eval_feature.unique_id)\n","            output = [convert_to_list(output[i]) for output in outputs]\n","            start_logits, end_logits, yes_logits, no_logits, unk_logits = output\n","            result = Result(unique_id=unique_id, start_logits=start_logits, end_logits=end_logits, yes_logits=yes_logits, no_logits=no_logits, unk_logits=unk_logits)\n","            mod_results.append(result)\n","\n","    # Get predictions for development dataset and store it in predictions.json\n","    output_prediction_file = os.path.join(output_directory+'/'+variant_name, \"predictions.json\")\n","    get_predictions(examples, features, mod_results, 20, 30, True, output_prediction_file, False, tokenizer)\n"]},{"cell_type":"markdown","metadata":{"id":"n-avtGOw9JEV"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o4RsfgerCeEp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1149d34a-1861-410d-b0fc-8e0938d70002"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertBaseUncasedModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n","- This IS expected if you are initializing BertBaseUncasedModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertBaseUncasedModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertBaseUncasedModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['fc.bias', 'linear1.weight', 'linear2.bias', 'linear2.weight', 'linear1.bias', 'fc.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Creating features from dataset file at /content/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data\n"]},{"output_type":"stream","name":"stderr","text":["Extracting features from dataset:  19%|█▉        | 1387/7199 [14:43<1:06:01,  1.47it/s]"]}],"source":["#   check if gpu is available to use it or not\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#   initialize configurations and tokenizer of Bert model \n","config = BertConfig.from_pretrained(pretrained_model, return_dict=False)\n","tokenizer = BertTokenizer.from_pretrained(pretrained_model)\n","\n","model = BertBaseUncasedModel.from_pretrained(pretrained_model, from_tf=bool(\".ckpt\" in pretrained_model), config=config,cache_dir=None,)\n","# print(model)\n","model.to(device)\n","\n","# if (os.path.exists(output_directory) and os.listdir(output_directory)):\n","#     raise ValueError(\"Output directory \" + output_directory + \" already exists, Change output_directory name\")\n","\n","#   Loading dataset and training\n","# this command will take several hours\n","cache_file_name = 'bert-base-uncased_train_with_T5_append'\n","\n","train_file_name = 'coqa-train-v1.0-append_with_T5.json'\n","\n","train_dataset = load_dataset(tokenizer, evaluate=False, cache_file_name=cache_file_name, train_file_name=train_file_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fvKBU3GgCeEr"},"outputs":[],"source":["# train the model on CoQA\n","train_loss = train(train_dataset, model, tokenizer, device)\n","\n","variant_name = 'Bert_with_T5_rewritten_epoch4'\n","\n","#   create output directory for model parameters and to write predictions\n","if not os.path.exists(output_directory+'/'+variant_name) :\n","    os.makedirs(output_directory+'/'+variant_name)\n","            \n","model_to_save = model.module if hasattr(model, \"module\") else model\n","model_to_save.save_pretrained(output_directory+'/'+variant_name)\n","tokenizer.save_pretrained(output_directory+'/'+variant_name)"]},{"cell_type":"markdown","metadata":{"id":"XDyYsqyW9JEY"},"source":["## Prediction\n","\n","predict on dev dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F_1nMH-SIivu"},"outputs":[],"source":["#   check if gpu is available to use it or not\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#   Loading Bert model for writing predictions\n","# model = BertBaseUncasedModel.from_pretrained(output_directory)\n","# tokenizer = BertTokenizer.from_pretrained(output_directory, do_lower_case=True)\n","# model.to(device)\n","# run for different parameters\n","\n","# check if gpu is available to use it or not\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# model_parameter_directory = [ f.path for f in os.scandir(output_directory) if f.is_dir() ]\n","\n","# for m in model_parameter_directory:\n","#     variant_name = m.split('/')[-1]\n","#     model = BertBaseUncasedModel.from_pretrained(m) \n","#     tokenizer = BertTokenizer.from_pretrained(m, do_lower_case=True)\n","#     model.to(device)\n","#     Write_predictions(model, tokenizer, device, variant_name)\n","# train_dataset = load_dataset(tokenizer, evaluate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":344804,"status":"ok","timestamp":1653380807028,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"g6PF9-kg9pvR","outputId":"3a5ce731-2a41-4345-fb58-3b1be61214ce"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/mydata/MyDrive/Colab Notebooks/data/bert-base-uncased_train\n","Loading cache /content/mydata/MyDrive/Colab Notebooks/data/bert-base-uncased_dev\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 536/536 [05:05<00:00,  1.76it/s]\n","Writing preditions: 100%|██████████| 7983/7983 [00:28<00:00, 283.33it/s]\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model_parameter_directory = [ f.path for f in os.scandir(output_directory) if f.is_dir() ]\n","\n","# for m in model_parameter_directory:\n","m = model_parameter_directory[1]\n","variant_name = m.split('/')[-1]\n","# m = m + '/pytorch_model_2.bin'\n","model = BertBaseUncasedModel.from_pretrained(m) \n","tokenizer = BertTokenizer.from_pretrained(m, do_lower_case=True)\n","model.to(device)\n","Write_predictions(model, tokenizer, device, variant_name)"]},{"cell_type":"markdown","metadata":{"id":"L_AFYP9k0Yu1"},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4VY9u1rP0cSu"},"outputs":[],"source":["evaluator = CoQAEvaluator(input_dir+'/'+predict_file)\n","\n","variant_name = 'Bert_from_original_Surya_epoch4'\n","\n","pre_file_bert = output_directory+'/'+variant_name+'/'+'predictions.json'\n","\n","# evaluate\n","with open(pre_file_bert) as f:\n","    pred_data = CoQAEvaluator.preds_to_dict(pre_file_bert)\n","\n","# write evaluate result\n","with open(output_directory+'/'+variant_name+'/'+'evaluation.json', 'w') as f:\n","    json.dump(evaluator.model_performance(pred_data), f, indent=2)\n","\n","# show\n","# print(json.dumps(evaluator.model_performance(pred_data), indent=2))"]},{"cell_type":"markdown","metadata":{"id":"QZt4mghfQyTj"},"source":["get_domain_scores --> model_performance --> get_raw_scores --> compute_turn_score --> _compute_turn_score --> compute_exact /  compute_f1\n"]},{"cell_type":"markdown","metadata":{"id":"N_xpwsOQpiva"},"source":["## Test any input on the fine-tuned model: By ZYZ"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_EAPoJPZAequ"},"outputs":[],"source":["### this cell load the fine tuned model\n","\n","# check if gpu is available to use it or not\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model_parameter_directory = [ f.path for f in os.scandir(output_directory) if f.is_dir() ]\n","\n","for m in model_parameter_directory:\n","    variant_name = m.split('/')[-1]\n","    model = BertBaseUncasedModel.from_pretrained(m) \n","    tokenizer = BertTokenizer.from_pretrained(m, do_lower_case=True)\n","    model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7961,"status":"ok","timestamp":1652290735026,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"Zy243hvSJv_J","outputId":"72eeff4e-6a9b-49a5-995e-a29851da5c20"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading cache /content/mydata/MyDrive/Colab Notebooks/data/bert-base-uncased_dev_test\n"]}],"source":["### load the dev dataset with gold answer\n","\n","cache_file = os.path.join(input_dir,\"bert-base-uncased_dev_test\")\n","\n","if os.path.exists(cache_file):\n","    print(\"Loading cache\",cache_file)\n","    features_and_dataset = torch.load(cache_file)\n","    features, dataset, examples = (\n","        features_and_dataset[\"features\"],features_and_dataset[\"dataset\"],features_and_dataset[\"examples\"])\n","else:\n","    print(\"Creating features from dataset file at\", input_dir)\n","\n","    processor = Processor()\n","\n","    examples = processor.get_examples(input_dir, 2, filename=predict_file, threads=1)\n","\n","    # max_seq_length is the total length for input sequence of BERT \n","    features, dataset = Extract_Features(examples=examples,tokenizer=tokenizer,max_seq_length=512, doc_stride=128, max_query_length=64, is_training=True, threads=1)\n","    #   caching it in a cache file to reduce time\n","    torch.save({\"features\": features, \"dataset\": dataset, \"examples\": examples}, cache_file)\n","\n","# create evaluation_test_dataloader\n","evalutation_test_sampler = SequentialSampler(dataset)\n","evaluation_test_dataloader = DataLoader(dataset, sampler=evalutation_test_sampler, batch_size=1)\n"]},{"cell_type":"markdown","metadata":{"id":"d_yk1-jrRIug"},"source":["1. model.eval() will notify all your layers that you are in eval mode, that way, batchnorm or dropout layers will work in eval mode instead of training mode.\n","2. torch.no_grad() impacts the autograd engine and deactivate it. It will reduce memory usage and speed up computations but you won’t be able to backprop (which you don’t want in an eval script)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4fa8h6OX1F0r"},"outputs":[],"source":["def predict(batch):\n","  '''\n","  given input_ids of text, QA history and current question, predict answer with \n","  model and compare with ground truth answer span\n","  '''\n","  model.eval()\n","  batch = tuple(t.to(device) for t in batch)\n","  with torch.no_grad():\n","      inputs = {\"input_ids\": batch[0],\"token_type_ids\": batch[1],\"attention_mask\": batch[2]}\n","      inputs_text = inputs[\"input_ids\"]\n","      outputs = model(**inputs)\n","\n","  # convert ids to texts\n","  input_ids = inputs_text[0]\n","  # tokens are all QA and text\n","  tokens = tokenizer.convert_ids_to_tokens(input_ids)\n","\n","  # sep_idx = list(input_ids).index(tokenizer.sep_token_id)\n","  # question = \" \".join(tokens[:(sep_idx+1)])\n","  # sep_idx2 = list(input_ids[(sep_idx+1):]).index(tokenizer.sep_token_id)\n","  # text = \" \".join(tokens[(sep_idx+1):(sep_idx2+(sep_idx+1))])\n","\n","  def convert_to_list(tensor):\n","      return tensor.detach().cpu().tolist()\n","      \n","  # extract answer from prediction of bert\n","  output = [convert_to_list(output[0]) for output in outputs]\n","  start_logits, end_logits, yes_logits, no_logits, unk_logits = output\n","  start_pos = np.argmax(start_logits)\n","  end_pos = np.argmax(end_logits)\n","\n","  print(\"\\nQuestion:\\n{}\".format(question.capitalize()))\n","  answer = \" \".join(tokens[start_pos:end_pos+1])\n","  print(\"Answer:\\n{}.\".format(answer.capitalize()))\n","\n","  # pass"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":226,"status":"ok","timestamp":1652291599956,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"hgpVaVcbk5oq","outputId":"50c69034-edd3-44b8-cba6-a38082ac1f73"},"outputs":[{"name":"stdout","output_type":"stream","text":["question: [CLS] where did she live ? in a barn [SEP] did she live alone ? no [SEP] who did she live with ? [SEP] \n","\n"," text: upon a time , in a barn near a farm house , there lived a little white kitten named cotton . cotton lived high up in a nice warm place above the barn where all of the farmer ' s horses slept . but cotton was n ' t alone in her little home above the barn , oh no . she shared her hay bed with her mommy and 5 other sisters . all of her sisters were cute and fluffy , like cotton . but she was the only white one in the bunch . the rest of her sisters were all orange with beautiful white tiger stripes like cotton ' s mommy . being different made cotton quite sad . she often wished she looked like the rest of her family . so one day , when cotton found a can of the old farmer ' s orange paint , she used it to paint herself like them . when her mommy and sisters found her they started laughing . \" what are you doing , cotton ? ! \" \" i only wanted to be more like you \" . cotton ' s mommy rubbed her face on cotton ' s and said \" oh cotton , but your fur is so pretty and special , like you . we would never want you to be any other way \" . and with that , cotton ' s mommy picked her up and dropped her into a big bucket of water . when cotton came out she was herself again . her sisters licked her face until cotton ' s fur was all all dry . \" do n ' t ever do that again , cotton ! \" they all cried . \" next time you might mess up that pretty white fur of yours and we would n ' t want that ! \" then cotton thought , \" i change my mind . i like being special \" . \n","\n"," gold answer: with her mommy and 5 other sisters\n"]}],"source":["random_num = np.random.randint(0,len(evaluation_test_dataloader))\n","random_num = 3\n","\n","## evaluation_test_dataloader is not iteratable\n","## dummy method\n","count = 0\n","for batch in evaluation_test_dataloader:\n","    # print(len(batch[3]))\n","    if count >= random_num:\n","        break\n","    count += 1\n","\n","# data from batch\n","input_ids = batch[0][0]\n","token_type_ids = batch[1][0]\n","tokens = tokenizer.convert_ids_to_tokens(input_ids)\n","\n","# sep_idx = list(input_ids).index(tokenizer.sep_token_id)\n","sep_idx = torch.where(token_type_ids==1)[0][0]\n","question = \" \".join(tokens[:(sep_idx)])\n","sep_idx2 = list(input_ids[(sep_idx+1):]).index(tokenizer.sep_token_id)\n","text = \" \".join(tokens[(sep_idx+1):(sep_idx2+(sep_idx+1))])\n","\n","# extract gold answer\n","start_positions = batch[3][0]\n","end_positions = batch[4][0]\n","gold_answer = \" \".join(tokens[start_positions:end_positions+1])\n","\n","print('question: '+question,'\\n\\n','text: '+text,'\\n\\n','gold answer: '+gold_answer)#+data[\"answer\"][random_num])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":256,"status":"ok","timestamp":1652291603571,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"z-1792vrN2hg","outputId":"946f244d-b798-4b70-cd90-6246ce9164b7"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Question:\n","[cls] where did she live ? in a barn [sep] did she live alone ? no [sep] who did she live with ? [sep]\n","Answer:\n","Her mommy and 5 other sisters.\n"]}],"source":["predict(batch)"]},{"cell_type":"markdown","metadata":{"id":"2JKyB4a6i0oh"},"source":["## Write all the failure cases"]},{"cell_type":"markdown","metadata":{"id":"EQpQpvMfRvOW"},"source":["#### Use evaluator.compute_turn_score to evaluate each case\n","\n","e.g.\n","\n","evaluator.compute_turn_score('3dr23u6we5exclen4th8uq9rb42tel', 4, 'her mommy and 5 other sisters')\n","\n","output: {'em': 1.0, 'f1': 1.0} (exact match and f1 score)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6gAzZ2Tdiu_Y"},"outputs":[],"source":["# failure criteria\n","criteria_flag = 'full'\n","\n","# tuples for all prediction data\n","pre_data_list = []\n","for key in pred_data.keys():\n","    pre_data_list.append(key + (pred_data[key],) )\n","\n","# find out all the failure cases: their ids\n","fail_results = []\n","for prediction in pre_data_list:\n","    result = evaluator.compute_turn_score(prediction[0],prediction[1],prediction[2])\n","    # totally wrong\n","    if criteria_flag == 'full':\n","        if result == {'em': 0.0, 'f1': 0.0}:\n","            fail_results.append(prediction)\n","    # not fully correct\n","    elif criteria_flag == 'partial':\n","        if result != {'em': 1.0, 'f1': 1.0}:\n","            fail_results.append(prediction)\n","    # print(result)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1652301186201,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"JJ5Jc5XUz9PU","outputId":"10ad3d83-4b0e-41cb-fdde-eacf2ac23b6d"},"outputs":[{"data":{"text/plain":["0.1663535011900288"]},"execution_count":130,"metadata":{},"output_type":"execute_result"}],"source":["len(fail_results) / len(pre_data_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jrOrErxuixSk"},"outputs":[],"source":["# write results\n","with open(output_directory+'/'+variant_name+'/'+'fully_failed_predictions.csv', mode='w+', encoding='utf-8-sig', newline='') as f:\n","    writer = csv.writer(f)\n","    writer.writerow(['passage_id','turn_id','story_type', 'story', 'history QA', 'current question', 'gold_answers', 'failed_prediction', 'scores'])\n","\n","with open(output_directory+'/'+variant_name+'/'+'fully_failed_predictions.csv', mode='a+', encoding='utf-8-sig', newline='') as f:\n","    writer = csv.writer(f)\n","    \n","    for fail_result in fail_results:\n","        story_id = fail_result[0]\n","        turn_id = fail_result[1]\n","        key = (story_id, turn_id)\n","        a_gold_list = evaluator.gold_data[key]\n","        scores = evaluator.compute_turn_score(fail_result[0],fail_result[1],fail_result[2])\n","        # evaluator.questions[turn_id]\n","        writer.writerow([story_id,turn_id,evaluator.id_to_source[story_id],evaluator.story_dict[story_id],\\\n","                         evaluator.question_dict[story_id][:turn_id-1],evaluator.question_dict[story_id][turn_id-1],a_gold_list, fail_result[2], scores])\n"]},{"cell_type":"markdown","metadata":{"id":"utcqU6v9xJva"},"source":["### Test results of yes/no answers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jzxs27q3xN0R"},"outputs":[],"source":["# tuples for all prediction data\n","yn_data_list = []\n","for key in pred_data.keys():\n","    if pred_data[key] in ['yes','no']:\n","        yn_data_list.append(key + (pred_data[key],) )\n","\n","\n","# find out all the failure cases: their ids\n","yn_fail_results = []\n","for prediction in yn_data_list:\n","    result = evaluator.compute_turn_score(prediction[0],prediction[1],prediction[2])\n","    # not fully correct\n","    # if result != {'em': 1.0, 'f1': 1.0}:\n","    # totally wrong\n","    if result == {'em': 0.0, 'f1': 0.0}:\n","        yn_fail_results.append(prediction)\n","    # print(result)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":227,"status":"ok","timestamp":1652301233734,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"dtBu46GBzQlL","outputId":"3d32d9a5-a928-45cb-f8c7-a12c6abcde45"},"outputs":[{"data":{"text/plain":["0.22059376174370537"]},"execution_count":133,"metadata":{},"output_type":"execute_result"}],"source":["yn_ratio = len(yn_data_list) / len(pred_data)\n","yn_ratio"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":304,"status":"ok","timestamp":1652301372268,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"K10NZwq81Wgr","outputId":"d68c3d5d-27e3-47bd-9cb9-0e0f724b6e45"},"outputs":[{"data":{"text/plain":["0.1486660237865638"]},"execution_count":135,"metadata":{},"output_type":"execute_result"}],"source":["non_yn_failure_ratio = (len(fail_results)-len(yn_fail_results)) / (len(pre_data_list)-len(yn_data_list))\n","non_yn_failure_ratio"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":234,"status":"ok","timestamp":1652301254288,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"LDQOn2EPzSDr","outputId":"917616ea-d4c6-4db8-eda5-b02196f9ae1a"},"outputs":[{"data":{"text/plain":["0.22884724588302102"]},"execution_count":134,"metadata":{},"output_type":"execute_result"}],"source":["yn_failure_ratio = len(yn_fail_results) / len(yn_data_list)\n","yn_failure_ratio"]},{"cell_type":"markdown","metadata":{"id":"kZTdQd0x1G9x"},"source":["Observation: The fully wrong ({'em': 0.0, 'f1': 0.0}) ratio of yes/no type question is higher then the normal question.\n","\n","\n","Need to construct a dictionary for computing the overall F1 score of these two different type of questions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nWGdX5rdz4CB"},"outputs":[],"source":["# tuples for all prediction data\n","yn_dict = {}\n","non_yn_dict = {}\n","yn_turns  = {}\n","for i in list(domain_mappings.values()):\n","    yn_turns[i] = 0\n","    \n","for key in pred_data.keys():\n","    if pred_data[key] in ['yes','no']:\n","        story_type = evaluator.id_to_source[key[0]]\n","        yn_turns[domain_mappings[story_type]] += 1\n","        yn_dict[key] = pred_data[key]\n","    else:\n","        non_yn_dict[key] = pred_data[key]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1652302902453,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"sms4ulTb67UH","outputId":"a7c53c24-0a2e-4159-9d4c-9c61399ea614"},"outputs":[{"data":{"text/plain":["['children_stories',\n"," 'literature',\n"," 'mid-high_school',\n"," 'news',\n"," 'wikipedia',\n"," 'science',\n"," 'reddit']"]},"execution_count":162,"metadata":{},"output_type":"execute_result"}],"source":["list(domain_mappings.values())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1652302594220,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"dKm0KKbU6LmW","outputId":"51ccad7f-b8fd-4441-bc37-1d9e1aa0235f"},"outputs":[{"data":{"text/plain":["290"]},"execution_count":155,"metadata":{},"output_type":"execute_result"}],"source":["yn_turns[story_type]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":970,"status":"ok","timestamp":1652303069989,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"e9hYB58U2TSO","outputId":"7b878721-8e5b-4db1-bb14-e332e1761ada"},"outputs":[{"name":"stdout","output_type":"stream","text":["no yes/no answer in reddit\n","no yes/no answer in science\n"]}],"source":["# evaluator.model_performance(yn_dict)\n","result_yn = json.dumps(evaluator.model_performance(yn_dict), indent=2)\n","result_yn = json.loads(result_yn)\n","for case in result_yn:\n","    if case not in ['in_domain', 'out_domain','overall']:\n","        if yn_turns[case] == 0:\n","            print('no yes/no answer in {}'.format(case))\n","            pass\n","        else:\n","            result_yn[case]['em'] = result_yn[case]['em'] * result_yn[case]['turns'] / yn_turns[case]\n","            result_yn[case]['f1'] = result_yn[case]['f1'] * result_yn[case]['turns'] / yn_turns[case]\n","            result_yn[case]['turns'] = yn_turns[case]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":243,"status":"ok","timestamp":1652303081553,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"6qSJkOVz8BB3","outputId":"85d37883-39da-49f1-b77b-7288e4501cd2"},"outputs":[{"data":{"text/plain":["{'children_stories': {'em': 74.27762039660055,\n","  'f1': 74.27762039660055,\n","  'turns': 353},\n"," 'in_domain': {'em': 16.8, 'f1': 16.8, 'turns': 7983},\n"," 'literature': {'em': 74.05432098765431,\n","  'f1': 74.05432098765431,\n","  'turns': 405},\n"," 'mid-high_school': {'em': 75.67442455242966,\n","  'f1': 75.67442455242966,\n","  'turns': 391},\n"," 'news': {'em': 78.35310559006211, 'f1': 78.35310559006211, 'turns': 322},\n"," 'out_domain': {'em': 0.0, 'f1': 0.0, 'turns': 0},\n"," 'overall': {'em': 16.8, 'f1': 16.8, 'turns': 7983},\n"," 'reddit': {'em': 0.0, 'f1': 0.0, 'turns': 0},\n"," 'science': {'em': 0.0, 'f1': 0.0, 'turns': 0},\n"," 'wikipedia': {'em': 80.17862068965518, 'f1': 80.17862068965518, 'turns': 290}}"]},"execution_count":169,"metadata":{},"output_type":"execute_result"}],"source":["result_yn"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3924,"status":"ok","timestamp":1652303205897,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"NicPvA_s7l6p","outputId":"4ad2cee6-5e96-4007-d613-80ea5357da35"},"outputs":[{"name":"stdout","output_type":"stream","text":["no yes/no answer in reddit\n","no yes/no answer in science\n"]}],"source":["# evaluator.model_performance(yn_dict)\n","result_non_yn = json.dumps(evaluator.model_performance(non_yn_dict), indent=2)\n","result_non_yn = json.loads(result_non_yn)\n","for case in result_non_yn:\n","    if case not in ['in_domain', 'out_domain','overall']:\n","        if yn_turns[case] == result_non_yn[case]['turns']:\n","            print('no yes/no answer in {}'.format(case))\n","            pass\n","        else:\n","            result_non_yn[case]['em'] = result_non_yn[case]['em'] * result_non_yn[case]['turns'] / (result_non_yn[case]['turns']-yn_turns[case])\n","            result_non_yn[case]['f1'] = result_non_yn[case]['f1'] * result_non_yn[case]['turns'] / (result_non_yn[case]['turns']-yn_turns[case])\n","            result_non_yn[case]['turns'] = (result_non_yn[case]['turns']-yn_turns[case])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":242,"status":"ok","timestamp":1652303212473,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"SUMq025_4CMy","outputId":"1022d8ac-0744-43d9-dac5-25f93289f5d2"},"outputs":[{"data":{"text/plain":["{'children_stories': {'em': 60.748600746268664,\n","  'f1': 73.64272388059702,\n","  'turns': 1072},\n"," 'in_domain': {'em': 48.1, 'f1': 58.1, 'turns': 7983},\n"," 'literature': {'em': 58.81306122448979,\n","  'f1': 71.98612244897959,\n","  'turns': 1225},\n"," 'mid-high_school': {'em': 57.63232963549921,\n","  'f1': 71.25451664025357,\n","  'turns': 1262},\n"," 'news': {'em': 63.62381311228335, 'f1': 76.92019593067067, 'turns': 1327},\n"," 'out_domain': {'em': 0.0, 'f1': 0.0, 'turns': 0},\n"," 'overall': {'em': 48.1, 'f1': 58.1, 'turns': 7983},\n"," 'reddit': {'em': 0.0, 'f1': 0.0, 'turns': 0},\n"," 'science': {'em': 0.0, 'f1': 0.0, 'turns': 0},\n"," 'wikipedia': {'em': 67.4254491017964, 'f1': 78.37904191616767, 'turns': 1336}}"]},"execution_count":172,"metadata":{},"output_type":"execute_result"}],"source":["result_non_yn"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4786,"status":"ok","timestamp":1652303268221,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"RQEzPaf04E59","outputId":"ca75b98c-5360-4f85-bd74-591582be8141"},"outputs":[{"name":"stdout","output_type":"stream","text":["{\n","  \"children_stories\": {\n","    \"em\": 64.1,\n","    \"f1\": 73.8,\n","    \"turns\": 1425\n","  },\n","  \"literature\": {\n","    \"em\": 62.5,\n","    \"f1\": 72.5,\n","    \"turns\": 1630\n","  },\n","  \"mid-high_school\": {\n","    \"em\": 61.9,\n","    \"f1\": 72.3,\n","    \"turns\": 1653\n","  },\n","  \"news\": {\n","    \"em\": 66.6,\n","    \"f1\": 77.2,\n","    \"turns\": 1649\n","  },\n","  \"wikipedia\": {\n","    \"em\": 69.6,\n","    \"f1\": 78.7,\n","    \"turns\": 1626\n","  },\n","  \"reddit\": {\n","    \"em\": 0.0,\n","    \"f1\": 0.0,\n","    \"turns\": 0\n","  },\n","  \"science\": {\n","    \"em\": 0.0,\n","    \"f1\": 0.0,\n","    \"turns\": 0\n","  },\n","  \"in_domain\": {\n","    \"em\": 65.0,\n","    \"f1\": 74.9,\n","    \"turns\": 7983\n","  },\n","  \"out_domain\": {\n","    \"em\": 0.0,\n","    \"f1\": 0.0,\n","    \"turns\": 0\n","  },\n","  \"overall\": {\n","    \"em\": 65.0,\n","    \"f1\": 74.9,\n","    \"turns\": 7983\n","  }\n","}\n"]}],"source":["print(json.dumps(evaluator.model_performance(pred_data), indent=2))"]},{"cell_type":"markdown","metadata":{"id":"4B63hRUr81dG"},"source":["so the actuall overall f1 score for yes/no question is higher"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Bert_convqa_with_evaluate.ipynb","provenance":[]},"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"},"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}