{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"JfgX6lC-CeEN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655734435600,"user_tz":-120,"elapsed":4226,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"}},"outputId":"cc7b4cdc-ac39-47bd-f13c-3d83f3f463c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Drive already mounted at ./mydata; to attempt to forcibly remount, call drive.mount(\"./mydata\", force_remount=True).\n"]}],"source":["!pip install transformers\n","from google.colab import drive\n","drive.mount('./mydata')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"wyzGkzsuHNAA","executionInfo":{"status":"ok","timestamp":1655734435600,"user_tz":-120,"elapsed":4,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"}}},"outputs":[],"source":["import sys\n","sys.path.append('./mydata/MyDrive/CSNLP_Project/Bert_model_COQA')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"UZAJcRiiEZqp","executionInfo":{"status":"ok","timestamp":1655734439957,"user_tz":-120,"elapsed":4361,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"}}},"outputs":[],"source":["import collections\n","import glob\n","import os\n","import torch\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from tqdm import tqdm, trange\n","from transformers import (AdamW, AutoConfig, AutoTokenizer, get_linear_schedule_with_warmup, BertTokenizer, BertModel, BertConfig)\n","from data.processors.coqa import Extract_Features, Processor, Result\n","from data.processors.evaluate import CoQAEvaluator, parse_args\n","from data.processors.Bert_model import BertBaseUncasedModel, load_dataset, Write_predictions\n","\n","import torch\n","import csv\n","import numpy as np\n","\n","import json\n","\n","# locations\n","train_file=\"coqa-train-v1.0.json\"\n","predict_file=\"coqa-dev-v1.0.json\"\n","cur_path = os.getcwd()\n","output_directory = cur_path + \"/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/Bert_models\"\n","input_dir = cur_path + \"/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data\"\n","# can use either BERT base or BERT large\n","pretrained_model=\"bert-base-uncased\"\n","# pretrained_model=\"bert-large-uncased\"\n","epochs = 4\n","evaluation_batch_size=16\n","train_batch_size=2"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"jx46LBgUCeEn","executionInfo":{"status":"ok","timestamp":1655734449534,"user_tz":-120,"elapsed":216,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"}}},"outputs":[],"source":["### train function\n","\n","def train(train_dataset, model, tokenizer, device):\n","\n","    train_sampler = RandomSampler(train_dataset) \n","    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=train_batch_size)\n","    t_total = len(train_dataloader) // 1 * epochs\n","\n","    # Preparing optimizer and scheduler\n","    \n","    optimizer_parameters = [{\"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in [\"bias\", \"LayerNorm.weight\"])],\"weight_decay\": 0.01,},{\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in [\"bias\", \"LayerNorm.weight\"])], \"weight_decay\": 0.0}]\n","    optimizer = AdamW(optimizer_parameters,lr=1e-5, eps=1e-8)\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=2000, num_training_steps=t_total)\n","\n","    # Check if saved optimizer or scheduler states exist\n","    if os.path.isfile(os.path.join(pretrained_model, \"optimizer.pt\")) and os.path.isfile(os.path.join(pretrained_model, \"scheduler.pt\")):\n","        optimizer.load_state_dict(torch.load(\n","            os.path.join(pretrained_model, \"optimizer.pt\")))\n","        scheduler.load_state_dict(torch.load(\n","            os.path.join(pretrained_model, \"scheduler.pt\")))\n","\n","    counter = 1\n","    epochs_trained = 0\n","    train_loss, loss = 0.0, 0.0\n","    model.zero_grad()\n","    iterator = trange(epochs_trained, int(epochs), desc=\"Epoch\", disable=False)\n","    for _ in iterator:\n","        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=True)\n","        for i,batch in enumerate(epoch_iterator):\n","            model.train()\n","            batch = tuple(t.to(device) for t in batch)\n","            inputs = { \"input_ids\": batch[0],\"token_type_ids\": batch[1], \"attention_mask\": batch[2],\"start_positions\": batch[3],\"end_positions\": batch[4],\"rational_mask\": batch[5],\"cls_idx\": batch[6]}\n","            # loss = model(**inputs, return_dict=False)\n","            loss = model(**inputs)\n","            loss.backward()\n","            train_loss += loss.item()\n","\n","            #   optimizing training parameters\n","            if (i + 1) % 1 == 0:\n","                optimizer.step()\n","                scheduler.step()  \n","                model.zero_grad()\n","                counter += 1\n","                #   Saving model weights every 1000 iterations\n","                if counter % 1000 == 0:\n","                    output_dir = os.path.join(output_directory, \"model_weights\"+str(epochs_trained))\n","                    if not os.path.exists(output_dir):\n","                        os.makedirs(output_dir)\n","                    model_to_save = model.module if hasattr(model, \"module\") else model\n","                    model_to_save.save_pretrained(output_dir)\n","                    tokenizer.save_pretrained(output_dir)\n","                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n","                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n","            if (i+1) % 1000 == 0:\n","                print('iter: {}, loss: {}'.format(i,train_loss/counter))\n","    return train_loss/counter"]},{"cell_type":"markdown","metadata":{"id":"n-avtGOw9JEV"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o4RsfgerCeEp"},"outputs":[],"source":["#   check if gpu is available to use it or not\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#   initialize configurations and tokenizer of Bert model \n","config = BertConfig.from_pretrained(pretrained_model, return_dict=False)\n","tokenizer = BertTokenizer.from_pretrained(pretrained_model)\n","\n","model = BertBaseUncasedModel.from_pretrained(pretrained_model, from_tf=bool(\".ckpt\" in pretrained_model), config=config,cache_dir=None,)\n","# print(model)\n","model.to(device)\n","\n","# if (os.path.exists(output_directory) and os.listdir(output_directory)):\n","#     raise ValueError(\"Output directory \" + output_directory + \" already exists, Change output_directory name\")\n","\n","method = 'replace'\n","\n","#   Loading dataset and training\n","# this command will take several hours\n","cache_file_name = 'bert-base-uncased_train_with_T5_{}'.format(method)\n","\n","train_file_name = 'coqa-train-v1.0-{}_with_T5.json'.format(method)\n","\n","train_dataset = load_dataset(tokenizer, input_dir=input_dir, evaluate=False, cache_file_name=cache_file_name, train_file_name=train_file_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fvKBU3GgCeEr"},"outputs":[],"source":["# train the model on CoQA\n","train_loss = train(train_dataset, model, tokenizer, device)\n","\n","variant_name = 'Bert_with_T5_rewritten_epoch4_replace'\n","\n","#   create output directory for model parameters and to write predictions\n","if not os.path.exists(output_directory+'/'+variant_name) :\n","    os.makedirs(output_directory+'/'+variant_name)\n","            \n","model_to_save = model.module if hasattr(model, \"module\") else model\n","model_to_save.save_pretrained(output_directory+'/'+variant_name)\n","tokenizer.save_pretrained(output_directory+'/'+variant_name)"]},{"cell_type":"markdown","metadata":{"id":"XDyYsqyW9JEY"},"source":["## Prediction\n","\n","predict on dev dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F_1nMH-SIivu"},"outputs":[],"source":["#   check if gpu is available to use it or not\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#   Loading Bert model for writing predictions\n","# model = BertBaseUncasedModel.from_pretrained(output_directory)\n","# tokenizer = BertTokenizer.from_pretrained(output_directory, do_lower_case=True)\n","# model.to(device)\n","# run for different parameters\n","\n","# check if gpu is available to use it or not\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# model_parameter_directory = [ f.path for f in os.scandir(output_directory) if f.is_dir() ]\n","\n","# for m in model_parameter_directory:\n","#     variant_name = m.split('/')[-1]\n","#     model = BertBaseUncasedModel.from_pretrained(m) \n","#     tokenizer = BertTokenizer.from_pretrained(m, do_lower_case=True)\n","#     model.to(device)\n","#     Write_predictions(model, tokenizer, device, variant_name)\n","# train_dataset = load_dataset(tokenizer, evaluate=False)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":243,"status":"ok","timestamp":1655734654176,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"g6PF9-kg9pvR","outputId":"48152db2-9716-4f52-e5d6-ae9cbba34a98"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/Bert_models/Bert_from_original_Surya_epoch4',\n"," '/content/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/Bert_models/Bert_with_T5_rewritten_epoch4',\n"," '/content/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/Bert_models/Bert_with_T5_rewritten_epoch4_replace']"]},"metadata":{},"execution_count":8}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model_parameter_directory = [ f.path for f in os.scandir(output_directory) if f.is_dir() ]\n","\n","cache_file_name = 'bert-base-uncased_dev_with_T5_append'\n","\n","train_file_name = 'coqa-dev-v1.0.json'\n","\n","model_parameter_directory"]},{"cell_type":"code","source":["# for m in model_parameter_directory:\n","m = model_parameter_directory[2]\n","variant_name = m.split('/')[-1]\n","# m = m + '/pytorch_model_2.bin'\n","model = BertBaseUncasedModel.from_pretrained(m) \n","tokenizer = BertTokenizer.from_pretrained(m, do_lower_case=True)\n","model.to(device)\n","Write_predictions(model, tokenizer, device, variant_name, input_dir=input_dir, output_directory=output_directory, cache_file_name=cache_file_name, predict_file_name=None)"],"metadata":{"id":"uOOl8SQ2s02q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655734894838,"user_tz":-120,"elapsed":236667,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"}},"outputId":"7b918754-f4e4-4293-e2da-26100a9b1ca1"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cache /content/mydata/MyDrive/CSNLP_Project/Bert_model_COQA/data/bert-base-uncased_dev_with_T5_append\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating: 100%|██████████| 8775/8775 [03:14<00:00, 45.15it/s]\n","Writing preditions: 100%|██████████| 7983/7983 [00:28<00:00, 284.79it/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"L_AFYP9k0Yu1"},"source":["## Evaluation"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"4VY9u1rP0cSu","executionInfo":{"status":"ok","timestamp":1655734947510,"user_tz":-120,"elapsed":5113,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"}}},"outputs":[],"source":["evaluator = CoQAEvaluator(input_dir+'/'+predict_file)\n","\n","variant_name = 'Bert_from_original_Surya_epoch4'\n","\n","pre_file_bert = output_directory+'/'+variant_name+'/'+'predictions.json'\n","\n","# evaluate\n","with open(pre_file_bert) as f:\n","    pred_data = CoQAEvaluator.preds_to_dict(pre_file_bert)\n","\n","# write evaluate result\n","with open(output_directory+'/'+variant_name+'/'+'evaluation.json', 'w') as f:\n","    json.dump(evaluator.model_performance(pred_data), f, indent=2)\n","\n","# show\n","# print(json.dumps(evaluator.model_performance(pred_data), indent=2))"]},{"cell_type":"markdown","metadata":{"id":"QZt4mghfQyTj"},"source":["get_domain_scores --> model_performance --> get_raw_scores --> compute_turn_score --> _compute_turn_score --> compute_exact /  compute_f1\n"]},{"cell_type":"markdown","metadata":{"id":"N_xpwsOQpiva"},"source":["## Test any input on the fine-tuned model: By ZYZ"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_EAPoJPZAequ"},"outputs":[],"source":["### this cell load the fine tuned model\n","\n","# check if gpu is available to use it or not\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model_parameter_directory = [ f.path for f in os.scandir(output_directory) if f.is_dir() ]\n","\n","for m in model_parameter_directory:\n","    variant_name = m.split('/')[-1]\n","    model = BertBaseUncasedModel.from_pretrained(m) \n","    tokenizer = BertTokenizer.from_pretrained(m, do_lower_case=True)\n","    model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7961,"status":"ok","timestamp":1652290735026,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"Zy243hvSJv_J","outputId":"72eeff4e-6a9b-49a5-995e-a29851da5c20"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading cache /content/mydata/MyDrive/Colab Notebooks/data/bert-base-uncased_dev_test\n"]}],"source":["### load the dev dataset with gold answer\n","\n","cache_file = os.path.join(input_dir,\"bert-base-uncased_dev_test\")\n","\n","if os.path.exists(cache_file):\n","    print(\"Loading cache\",cache_file)\n","    features_and_dataset = torch.load(cache_file)\n","    features, dataset, examples = (\n","        features_and_dataset[\"features\"],features_and_dataset[\"dataset\"],features_and_dataset[\"examples\"])\n","else:\n","    print(\"Creating features from dataset file at\", input_dir)\n","\n","    processor = Processor()\n","\n","    examples = processor.get_examples(input_dir, 2, filename=predict_file, threads=1)\n","\n","    # max_seq_length is the total length for input sequence of BERT \n","    features, dataset = Extract_Features(examples=examples,tokenizer=tokenizer,max_seq_length=512, doc_stride=128, max_query_length=64, is_training=True, threads=1)\n","    #   caching it in a cache file to reduce time\n","    torch.save({\"features\": features, \"dataset\": dataset, \"examples\": examples}, cache_file)\n","\n","# create evaluation_test_dataloader\n","evalutation_test_sampler = SequentialSampler(dataset)\n","evaluation_test_dataloader = DataLoader(dataset, sampler=evalutation_test_sampler, batch_size=1)\n"]},{"cell_type":"markdown","metadata":{"id":"d_yk1-jrRIug"},"source":["1. model.eval() will notify all your layers that you are in eval mode, that way, batchnorm or dropout layers will work in eval mode instead of training mode.\n","2. torch.no_grad() impacts the autograd engine and deactivate it. It will reduce memory usage and speed up computations but you won’t be able to backprop (which you don’t want in an eval script)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4fa8h6OX1F0r"},"outputs":[],"source":["def predict(batch):\n","  '''\n","  given input_ids of text, QA history and current question, predict answer with \n","  model and compare with ground truth answer span\n","  '''\n","  model.eval()\n","  batch = tuple(t.to(device) for t in batch)\n","  with torch.no_grad():\n","      inputs = {\"input_ids\": batch[0],\"token_type_ids\": batch[1],\"attention_mask\": batch[2]}\n","      inputs_text = inputs[\"input_ids\"]\n","      outputs = model(**inputs)\n","\n","  # convert ids to texts\n","  input_ids = inputs_text[0]\n","  # tokens are all QA and text\n","  tokens = tokenizer.convert_ids_to_tokens(input_ids)\n","\n","  # sep_idx = list(input_ids).index(tokenizer.sep_token_id)\n","  # question = \" \".join(tokens[:(sep_idx+1)])\n","  # sep_idx2 = list(input_ids[(sep_idx+1):]).index(tokenizer.sep_token_id)\n","  # text = \" \".join(tokens[(sep_idx+1):(sep_idx2+(sep_idx+1))])\n","\n","  def convert_to_list(tensor):\n","      return tensor.detach().cpu().tolist()\n","      \n","  # extract answer from prediction of bert\n","  output = [convert_to_list(output[0]) for output in outputs]\n","  start_logits, end_logits, yes_logits, no_logits, unk_logits = output\n","  start_pos = np.argmax(start_logits)\n","  end_pos = np.argmax(end_logits)\n","\n","  print(\"\\nQuestion:\\n{}\".format(question.capitalize()))\n","  answer = \" \".join(tokens[start_pos:end_pos+1])\n","  print(\"Answer:\\n{}.\".format(answer.capitalize()))\n","\n","  # pass"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":226,"status":"ok","timestamp":1652291599956,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"hgpVaVcbk5oq","outputId":"50c69034-edd3-44b8-cba6-a38082ac1f73"},"outputs":[{"name":"stdout","output_type":"stream","text":["question: [CLS] where did she live ? in a barn [SEP] did she live alone ? no [SEP] who did she live with ? [SEP] \n","\n"," text: upon a time , in a barn near a farm house , there lived a little white kitten named cotton . cotton lived high up in a nice warm place above the barn where all of the farmer ' s horses slept . but cotton was n ' t alone in her little home above the barn , oh no . she shared her hay bed with her mommy and 5 other sisters . all of her sisters were cute and fluffy , like cotton . but she was the only white one in the bunch . the rest of her sisters were all orange with beautiful white tiger stripes like cotton ' s mommy . being different made cotton quite sad . she often wished she looked like the rest of her family . so one day , when cotton found a can of the old farmer ' s orange paint , she used it to paint herself like them . when her mommy and sisters found her they started laughing . \" what are you doing , cotton ? ! \" \" i only wanted to be more like you \" . cotton ' s mommy rubbed her face on cotton ' s and said \" oh cotton , but your fur is so pretty and special , like you . we would never want you to be any other way \" . and with that , cotton ' s mommy picked her up and dropped her into a big bucket of water . when cotton came out she was herself again . her sisters licked her face until cotton ' s fur was all all dry . \" do n ' t ever do that again , cotton ! \" they all cried . \" next time you might mess up that pretty white fur of yours and we would n ' t want that ! \" then cotton thought , \" i change my mind . i like being special \" . \n","\n"," gold answer: with her mommy and 5 other sisters\n"]}],"source":["random_num = np.random.randint(0,len(evaluation_test_dataloader))\n","random_num = 3\n","\n","## evaluation_test_dataloader is not iteratable\n","## dummy method\n","count = 0\n","for batch in evaluation_test_dataloader:\n","    # print(len(batch[3]))\n","    if count >= random_num:\n","        break\n","    count += 1\n","\n","# data from batch\n","input_ids = batch[0][0]\n","token_type_ids = batch[1][0]\n","tokens = tokenizer.convert_ids_to_tokens(input_ids)\n","\n","# sep_idx = list(input_ids).index(tokenizer.sep_token_id)\n","sep_idx = torch.where(token_type_ids==1)[0][0]\n","question = \" \".join(tokens[:(sep_idx)])\n","sep_idx2 = list(input_ids[(sep_idx+1):]).index(tokenizer.sep_token_id)\n","text = \" \".join(tokens[(sep_idx+1):(sep_idx2+(sep_idx+1))])\n","\n","# extract gold answer\n","start_positions = batch[3][0]\n","end_positions = batch[4][0]\n","gold_answer = \" \".join(tokens[start_positions:end_positions+1])\n","\n","print('question: '+question,'\\n\\n','text: '+text,'\\n\\n','gold answer: '+gold_answer)#+data[\"answer\"][random_num])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":256,"status":"ok","timestamp":1652291603571,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"z-1792vrN2hg","outputId":"946f244d-b798-4b70-cd90-6246ce9164b7"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Question:\n","[cls] where did she live ? in a barn [sep] did she live alone ? no [sep] who did she live with ? [sep]\n","Answer:\n","Her mommy and 5 other sisters.\n"]}],"source":["predict(batch)"]},{"cell_type":"markdown","metadata":{"id":"2JKyB4a6i0oh"},"source":["## Write all the failure cases"]},{"cell_type":"markdown","metadata":{"id":"EQpQpvMfRvOW"},"source":["#### Use evaluator.compute_turn_score to evaluate each case\n","\n","e.g.\n","\n","evaluator.compute_turn_score('3dr23u6we5exclen4th8uq9rb42tel', 4, 'her mommy and 5 other sisters')\n","\n","output: {'em': 1.0, 'f1': 1.0} (exact match and f1 score)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6gAzZ2Tdiu_Y"},"outputs":[],"source":["# failure criteria\n","criteria_flag = 'full'\n","\n","# tuples for all prediction data\n","pre_data_list = []\n","for key in pred_data.keys():\n","    pre_data_list.append(key + (pred_data[key],) )\n","\n","# find out all the failure cases: their ids\n","fail_results = []\n","for prediction in pre_data_list:\n","    result = evaluator.compute_turn_score(prediction[0],prediction[1],prediction[2])\n","    # totally wrong\n","    if criteria_flag == 'full':\n","        if result == {'em': 0.0, 'f1': 0.0}:\n","            fail_results.append(prediction)\n","    # not fully correct\n","    elif criteria_flag == 'partial':\n","        if result != {'em': 1.0, 'f1': 1.0}:\n","            fail_results.append(prediction)\n","    # print(result)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1652301186201,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"JJ5Jc5XUz9PU","outputId":"10ad3d83-4b0e-41cb-fdde-eacf2ac23b6d"},"outputs":[{"data":{"text/plain":["0.1663535011900288"]},"execution_count":130,"metadata":{},"output_type":"execute_result"}],"source":["len(fail_results) / len(pre_data_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jrOrErxuixSk"},"outputs":[],"source":["# write results\n","with open(output_directory+'/'+variant_name+'/'+'fully_failed_predictions.csv', mode='w+', encoding='utf-8-sig', newline='') as f:\n","    writer = csv.writer(f)\n","    writer.writerow(['passage_id','turn_id','story_type', 'story', 'history QA', 'current question', 'gold_answers', 'failed_prediction', 'scores'])\n","\n","with open(output_directory+'/'+variant_name+'/'+'fully_failed_predictions.csv', mode='a+', encoding='utf-8-sig', newline='') as f:\n","    writer = csv.writer(f)\n","    \n","    for fail_result in fail_results:\n","        story_id = fail_result[0]\n","        turn_id = fail_result[1]\n","        key = (story_id, turn_id)\n","        a_gold_list = evaluator.gold_data[key]\n","        scores = evaluator.compute_turn_score(fail_result[0],fail_result[1],fail_result[2])\n","        # evaluator.questions[turn_id]\n","        writer.writerow([story_id,turn_id,evaluator.id_to_source[story_id],evaluator.story_dict[story_id],\\\n","                         evaluator.question_dict[story_id][:turn_id-1],evaluator.question_dict[story_id][turn_id-1],a_gold_list, fail_result[2], scores])\n"]},{"cell_type":"markdown","metadata":{"id":"utcqU6v9xJva"},"source":["### Test results of yes/no answers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jzxs27q3xN0R"},"outputs":[],"source":["# tuples for all prediction data\n","yn_data_list = []\n","for key in pred_data.keys():\n","    if pred_data[key] in ['yes','no']:\n","        yn_data_list.append(key + (pred_data[key],) )\n","\n","\n","# find out all the failure cases: their ids\n","yn_fail_results = []\n","for prediction in yn_data_list:\n","    result = evaluator.compute_turn_score(prediction[0],prediction[1],prediction[2])\n","    # not fully correct\n","    # if result != {'em': 1.0, 'f1': 1.0}:\n","    # totally wrong\n","    if result == {'em': 0.0, 'f1': 0.0}:\n","        yn_fail_results.append(prediction)\n","    # print(result)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":227,"status":"ok","timestamp":1652301233734,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"dtBu46GBzQlL","outputId":"3d32d9a5-a928-45cb-f8c7-a12c6abcde45"},"outputs":[{"data":{"text/plain":["0.22059376174370537"]},"execution_count":133,"metadata":{},"output_type":"execute_result"}],"source":["yn_ratio = len(yn_data_list) / len(pred_data)\n","yn_ratio"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":304,"status":"ok","timestamp":1652301372268,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"K10NZwq81Wgr","outputId":"d68c3d5d-27e3-47bd-9cb9-0e0f724b6e45"},"outputs":[{"data":{"text/plain":["0.1486660237865638"]},"execution_count":135,"metadata":{},"output_type":"execute_result"}],"source":["non_yn_failure_ratio = (len(fail_results)-len(yn_fail_results)) / (len(pre_data_list)-len(yn_data_list))\n","non_yn_failure_ratio"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":234,"status":"ok","timestamp":1652301254288,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"LDQOn2EPzSDr","outputId":"917616ea-d4c6-4db8-eda5-b02196f9ae1a"},"outputs":[{"data":{"text/plain":["0.22884724588302102"]},"execution_count":134,"metadata":{},"output_type":"execute_result"}],"source":["yn_failure_ratio = len(yn_fail_results) / len(yn_data_list)\n","yn_failure_ratio"]},{"cell_type":"markdown","metadata":{"id":"kZTdQd0x1G9x"},"source":["Observation: The fully wrong ({'em': 0.0, 'f1': 0.0}) ratio of yes/no type question is higher then the normal question.\n","\n","\n","Need to construct a dictionary for computing the overall F1 score of these two different type of questions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nWGdX5rdz4CB"},"outputs":[],"source":["# tuples for all prediction data\n","yn_dict = {}\n","non_yn_dict = {}\n","yn_turns  = {}\n","for i in list(domain_mappings.values()):\n","    yn_turns[i] = 0\n","    \n","for key in pred_data.keys():\n","    if pred_data[key] in ['yes','no']:\n","        story_type = evaluator.id_to_source[key[0]]\n","        yn_turns[domain_mappings[story_type]] += 1\n","        yn_dict[key] = pred_data[key]\n","    else:\n","        non_yn_dict[key] = pred_data[key]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1652302902453,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"sms4ulTb67UH","outputId":"a7c53c24-0a2e-4159-9d4c-9c61399ea614"},"outputs":[{"data":{"text/plain":["['children_stories',\n"," 'literature',\n"," 'mid-high_school',\n"," 'news',\n"," 'wikipedia',\n"," 'science',\n"," 'reddit']"]},"execution_count":162,"metadata":{},"output_type":"execute_result"}],"source":["list(domain_mappings.values())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1652302594220,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"dKm0KKbU6LmW","outputId":"51ccad7f-b8fd-4441-bc37-1d9e1aa0235f"},"outputs":[{"data":{"text/plain":["290"]},"execution_count":155,"metadata":{},"output_type":"execute_result"}],"source":["yn_turns[story_type]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":970,"status":"ok","timestamp":1652303069989,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"e9hYB58U2TSO","outputId":"7b878721-8e5b-4db1-bb14-e332e1761ada"},"outputs":[{"name":"stdout","output_type":"stream","text":["no yes/no answer in reddit\n","no yes/no answer in science\n"]}],"source":["# evaluator.model_performance(yn_dict)\n","result_yn = json.dumps(evaluator.model_performance(yn_dict), indent=2)\n","result_yn = json.loads(result_yn)\n","for case in result_yn:\n","    if case not in ['in_domain', 'out_domain','overall']:\n","        if yn_turns[case] == 0:\n","            print('no yes/no answer in {}'.format(case))\n","            pass\n","        else:\n","            result_yn[case]['em'] = result_yn[case]['em'] * result_yn[case]['turns'] / yn_turns[case]\n","            result_yn[case]['f1'] = result_yn[case]['f1'] * result_yn[case]['turns'] / yn_turns[case]\n","            result_yn[case]['turns'] = yn_turns[case]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":243,"status":"ok","timestamp":1652303081553,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"6qSJkOVz8BB3","outputId":"85d37883-39da-49f1-b77b-7288e4501cd2"},"outputs":[{"data":{"text/plain":["{'children_stories': {'em': 74.27762039660055,\n","  'f1': 74.27762039660055,\n","  'turns': 353},\n"," 'in_domain': {'em': 16.8, 'f1': 16.8, 'turns': 7983},\n"," 'literature': {'em': 74.05432098765431,\n","  'f1': 74.05432098765431,\n","  'turns': 405},\n"," 'mid-high_school': {'em': 75.67442455242966,\n","  'f1': 75.67442455242966,\n","  'turns': 391},\n"," 'news': {'em': 78.35310559006211, 'f1': 78.35310559006211, 'turns': 322},\n"," 'out_domain': {'em': 0.0, 'f1': 0.0, 'turns': 0},\n"," 'overall': {'em': 16.8, 'f1': 16.8, 'turns': 7983},\n"," 'reddit': {'em': 0.0, 'f1': 0.0, 'turns': 0},\n"," 'science': {'em': 0.0, 'f1': 0.0, 'turns': 0},\n"," 'wikipedia': {'em': 80.17862068965518, 'f1': 80.17862068965518, 'turns': 290}}"]},"execution_count":169,"metadata":{},"output_type":"execute_result"}],"source":["result_yn"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3924,"status":"ok","timestamp":1652303205897,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"NicPvA_s7l6p","outputId":"4ad2cee6-5e96-4007-d613-80ea5357da35"},"outputs":[{"name":"stdout","output_type":"stream","text":["no yes/no answer in reddit\n","no yes/no answer in science\n"]}],"source":["# evaluator.model_performance(yn_dict)\n","result_non_yn = json.dumps(evaluator.model_performance(non_yn_dict), indent=2)\n","result_non_yn = json.loads(result_non_yn)\n","for case in result_non_yn:\n","    if case not in ['in_domain', 'out_domain','overall']:\n","        if yn_turns[case] == result_non_yn[case]['turns']:\n","            print('no yes/no answer in {}'.format(case))\n","            pass\n","        else:\n","            result_non_yn[case]['em'] = result_non_yn[case]['em'] * result_non_yn[case]['turns'] / (result_non_yn[case]['turns']-yn_turns[case])\n","            result_non_yn[case]['f1'] = result_non_yn[case]['f1'] * result_non_yn[case]['turns'] / (result_non_yn[case]['turns']-yn_turns[case])\n","            result_non_yn[case]['turns'] = (result_non_yn[case]['turns']-yn_turns[case])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":242,"status":"ok","timestamp":1652303212473,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"SUMq025_4CMy","outputId":"1022d8ac-0744-43d9-dac5-25f93289f5d2"},"outputs":[{"data":{"text/plain":["{'children_stories': {'em': 60.748600746268664,\n","  'f1': 73.64272388059702,\n","  'turns': 1072},\n"," 'in_domain': {'em': 48.1, 'f1': 58.1, 'turns': 7983},\n"," 'literature': {'em': 58.81306122448979,\n","  'f1': 71.98612244897959,\n","  'turns': 1225},\n"," 'mid-high_school': {'em': 57.63232963549921,\n","  'f1': 71.25451664025357,\n","  'turns': 1262},\n"," 'news': {'em': 63.62381311228335, 'f1': 76.92019593067067, 'turns': 1327},\n"," 'out_domain': {'em': 0.0, 'f1': 0.0, 'turns': 0},\n"," 'overall': {'em': 48.1, 'f1': 58.1, 'turns': 7983},\n"," 'reddit': {'em': 0.0, 'f1': 0.0, 'turns': 0},\n"," 'science': {'em': 0.0, 'f1': 0.0, 'turns': 0},\n"," 'wikipedia': {'em': 67.4254491017964, 'f1': 78.37904191616767, 'turns': 1336}}"]},"execution_count":172,"metadata":{},"output_type":"execute_result"}],"source":["result_non_yn"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4786,"status":"ok","timestamp":1652303268221,"user":{"displayName":"Mr. Z","userId":"12849447431415807254"},"user_tz":-120},"id":"RQEzPaf04E59","outputId":"ca75b98c-5360-4f85-bd74-591582be8141"},"outputs":[{"name":"stdout","output_type":"stream","text":["{\n","  \"children_stories\": {\n","    \"em\": 64.1,\n","    \"f1\": 73.8,\n","    \"turns\": 1425\n","  },\n","  \"literature\": {\n","    \"em\": 62.5,\n","    \"f1\": 72.5,\n","    \"turns\": 1630\n","  },\n","  \"mid-high_school\": {\n","    \"em\": 61.9,\n","    \"f1\": 72.3,\n","    \"turns\": 1653\n","  },\n","  \"news\": {\n","    \"em\": 66.6,\n","    \"f1\": 77.2,\n","    \"turns\": 1649\n","  },\n","  \"wikipedia\": {\n","    \"em\": 69.6,\n","    \"f1\": 78.7,\n","    \"turns\": 1626\n","  },\n","  \"reddit\": {\n","    \"em\": 0.0,\n","    \"f1\": 0.0,\n","    \"turns\": 0\n","  },\n","  \"science\": {\n","    \"em\": 0.0,\n","    \"f1\": 0.0,\n","    \"turns\": 0\n","  },\n","  \"in_domain\": {\n","    \"em\": 65.0,\n","    \"f1\": 74.9,\n","    \"turns\": 7983\n","  },\n","  \"out_domain\": {\n","    \"em\": 0.0,\n","    \"f1\": 0.0,\n","    \"turns\": 0\n","  },\n","  \"overall\": {\n","    \"em\": 65.0,\n","    \"f1\": 74.9,\n","    \"turns\": 7983\n","  }\n","}\n"]}],"source":["print(json.dumps(evaluator.model_performance(pred_data), indent=2))"]},{"cell_type":"markdown","metadata":{"id":"4B63hRUr81dG"},"source":["so the actuall overall f1 score for yes/no question is higher"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Bert_convqa_with_evaluate.ipynb","provenance":[]},"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"},"kernelspec":{"display_name":"Python 3.8.10 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}